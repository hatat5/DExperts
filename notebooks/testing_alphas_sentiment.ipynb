{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import json\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentiment_results(models_dict, max_gens=None, auto_eval=False):\n",
    "    res = {}\n",
    "    for model in tqdm(models_dict):\n",
    "        if os.path.exists(models_dict[model]):\n",
    "            df = pd.read_json(models_dict[model], lines=True)[:max_gens]\n",
    "            sentiment_labels = df.generations.apply(lambda x: [y['label'] for y in x])\n",
    "            positive_proportion = sentiment_labels.apply(lambda x: np.sum([1 for y in x if y == 'POSITIVE'])/len(x))\n",
    "            res[model] = {\n",
    "                'positive_proportion': positive_proportion.mean()\n",
    "            }\n",
    "        else:\n",
    "            res[model] = {\n",
    "                'positive_proportion': -1\n",
    "            }\n",
    "        if auto_eval:\n",
    "            #read automatic evaluation\n",
    "            with open(Path(os.path.dirname(models_dict[model])) / 'eval_results.txt', 'r') as fo:\n",
    "                for i, line in enumerate(fo):\n",
    "                    if i < 3:\n",
    "                        dist_n = float(line.rstrip().replace(f'dist-{i+1} = ', ''))\n",
    "                        res[model][f'dist-{i+1}'] = dist_n\n",
    "                    elif i == 3:\n",
    "                        ppl = float(line.replace('perplexity = ', ''))\n",
    "                        res[model]['perplexity'] = ppl\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(neutral_prompts_res, adversarial_prompts_res, key):\n",
    "    \"\"\"\n",
    "    return weighted average of dist-n or perplexity value across neural prompts (5k) and adversarial prompts (2.5k)\n",
    "    \"\"\"\n",
    "    return np.average([neutral_prompts_res[model][key], adversarial_prompts_res[model][key]], weights=[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:03<00:00, 92.26it/s] \n"
     ]
    }
   ],
   "source": [
    "POS_DIR = Path('../generations/sentiment/positive_prompts_mini/')\n",
    "\n",
    "alpha_bases = [1.0]\n",
    "alpha_experts = [0, 0.25, 0.33, 0.5, 1.0, 1.5, 2.0, 3.0, 5.0, 10.0, 20.0, 40.0, 80.0, 160.0, 320.0]\n",
    "alpha_antiexperts = [0, -0.25, -0.33, -0.5, -1.0, -1.5, -2.0, -3.0, -5.0, -10.0, -20.0]\n",
    "layer_nums = [12, 22]\n",
    "configs = ['dexperts_steer']\n",
    "\n",
    "models = {}\n",
    "\n",
    "# baselines\n",
    "models['dexperts_alphas_1_3.2_-3.2 (on full data)'] = {\n",
    "    'pos_path': f'../generations/sentiment/positive_prompts/large_experts/negative/dexperts/alphas_1.0_3.2_-3.2/prompted_gens_dexperts.jsonl',\n",
    "}\n",
    "\n",
    "models['dexperts_alphas_1_3.2_-3.2'] = {\n",
    "    'pos_path': f'../generations/sentiment/positive_prompts_mini/large_experts/negative/dexperts/prompted_gens_dexperts.jsonl',\n",
    "}\n",
    "\n",
    "#models['dexperts_antionly_alphas_1_3.2_-3.2'] = {\n",
    "#    'pos_path': POS_DIR / f'large_experts/negative/dexperts_anti_only/prompted_gens_dexperts.jsonl',\n",
    "#}\n",
    "\n",
    "# Experimental configs\n",
    "for alpha_base in alpha_bases:\n",
    "    for alpha_expert in alpha_experts:\n",
    "        for alpha_antiexpert in alpha_antiexperts:\n",
    "            for layer_num in layer_nums:\n",
    "                for config in configs:\n",
    "                    model_name = f\"{config}_layer{layer_num}_alphas_{alpha_base}_{alpha_expert}_{alpha_antiexpert}\"\n",
    "                    if config == 'dexperts_steer':\n",
    "                        modification = '_with_expert'\n",
    "                    else:\n",
    "                        modification = ''\n",
    "\n",
    "                    models[model_name] = {\n",
    "                        'pos_path': POS_DIR / f'large_experts/negative/{config}/steering_large_gpt2/alphas_{alpha_base}_{alpha_expert}_{alpha_antiexpert}/layer_{layer_num}_freeze_emb_lm_head{modification}_epochs100_train_val_sst/combine_at_layer/prompted_gens_dexperts-steer.jsonl'\n",
    "                    }\n",
    "\n",
    "pos_prompts_res = read_sentiment_results({m: p['pos_path'] for m,p in models.items()}, auto_eval=True)\n",
    "    \n",
    "negative_steering_res = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_prompts</th>\n",
       "      <th>dist-1</th>\n",
       "      <th>dist-2</th>\n",
       "      <th>dist-3</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.25_0</th>\n",
       "      <td>94.80</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>41.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.33_0</th>\n",
       "      <td>94.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>36.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.5_0</th>\n",
       "      <td>94.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>34.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.5_-0.33</th>\n",
       "      <td>94.40</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>43.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.5_-0.25</th>\n",
       "      <td>94.08</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>42.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.33_-0.25</th>\n",
       "      <td>93.76</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>53.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.0_-0.33</th>\n",
       "      <td>93.52</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>33.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_3.0_-20.0</th>\n",
       "      <td>93.20</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.72</td>\n",
       "      <td>6113.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0_0</th>\n",
       "      <td>93.20</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>70.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.33_0</th>\n",
       "      <td>93.12</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>668.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_2.0_-20.0</th>\n",
       "      <td>93.12</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.73</td>\n",
       "      <td>6164.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_5.0_-20.0</th>\n",
       "      <td>93.12</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.71</td>\n",
       "      <td>5956.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.0_-0.5</th>\n",
       "      <td>93.12</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>36.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.0_-0.25</th>\n",
       "      <td>93.04</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>33.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.25_-10.0</th>\n",
       "      <td>92.88</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.73</td>\n",
       "      <td>6035.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.5_-20.0</th>\n",
       "      <td>92.80</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.73</td>\n",
       "      <td>6216.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.0_-1.5</th>\n",
       "      <td>92.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1873.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_-10.0</th>\n",
       "      <td>92.64</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.73</td>\n",
       "      <td>5922.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.33_-10.0</th>\n",
       "      <td>92.64</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6036.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.33_-0.33</th>\n",
       "      <td>92.56</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>67.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_-20.0</th>\n",
       "      <td>92.48</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6185.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.5_-1.0</th>\n",
       "      <td>92.48</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>38.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.5_0</th>\n",
       "      <td>92.40</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>495.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.5_-10.0</th>\n",
       "      <td>92.40</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6071.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.33_-1.0</th>\n",
       "      <td>92.40</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.85</td>\n",
       "      <td>3959.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.5_-20.0</th>\n",
       "      <td>92.40</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6185.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.5_-0.5</th>\n",
       "      <td>92.32</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>71.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.0_0</th>\n",
       "      <td>92.32</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>34.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.25_-20.0</th>\n",
       "      <td>92.24</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6174.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_-20.0</th>\n",
       "      <td>92.24</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6097.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.33_-20.0</th>\n",
       "      <td>92.16</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6241.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0_-0.5</th>\n",
       "      <td>92.08</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1740.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.25_-0.25</th>\n",
       "      <td>92.08</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>68.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.33_-0.25</th>\n",
       "      <td>92.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1036.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.5_-0.5</th>\n",
       "      <td>92.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>33.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.5_-2.0</th>\n",
       "      <td>91.92</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.88</td>\n",
       "      <td>4209.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.5_-0.25</th>\n",
       "      <td>91.92</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>34.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.5_0</th>\n",
       "      <td>91.84</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>35.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_0</th>\n",
       "      <td>91.84</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1093.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.25_-1.5</th>\n",
       "      <td>91.68</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6143.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_-0.33</th>\n",
       "      <td>91.60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>403.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.5_-0.33</th>\n",
       "      <td>91.52</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>34.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.33_-3.0</th>\n",
       "      <td>91.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4655.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.0_-1.0</th>\n",
       "      <td>91.44</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>78.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_-1.5</th>\n",
       "      <td>91.44</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4385.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_2.0_-1.5</th>\n",
       "      <td>91.44</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>41.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.33_-0.5</th>\n",
       "      <td>91.36</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1522.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.5_-0.25</th>\n",
       "      <td>91.36</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.86</td>\n",
       "      <td>794.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_-0.33</th>\n",
       "      <td>91.28</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2339.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.5_-2.0</th>\n",
       "      <td>91.28</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>2156.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.25_-0.33</th>\n",
       "      <td>91.20</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>99.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.5_-1.0</th>\n",
       "      <td>91.12</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1803.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_2.0_-1.0</th>\n",
       "      <td>91.12</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>34.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_-10.0</th>\n",
       "      <td>91.04</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6214.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.25_0</th>\n",
       "      <td>91.04</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.86</td>\n",
       "      <td>782.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.33_-0.33</th>\n",
       "      <td>90.88</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1098.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_0</th>\n",
       "      <td>90.88</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>291.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.5_0</th>\n",
       "      <td>90.88</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>225.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.5_-0.5</th>\n",
       "      <td>90.72</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1116.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.33_-2.0</th>\n",
       "      <td>90.72</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>4608.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_-0.5</th>\n",
       "      <td>90.72</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.87</td>\n",
       "      <td>3007.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.25_-0.25</th>\n",
       "      <td>90.64</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1081.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.5_-10.0</th>\n",
       "      <td>90.56</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.76</td>\n",
       "      <td>6231.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_-0.25</th>\n",
       "      <td>90.48</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>356.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.25_-3.0</th>\n",
       "      <td>90.40</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5319.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.5_-3.0</th>\n",
       "      <td>90.32</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5537.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_2.0_-0.25</th>\n",
       "      <td>90.32</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>37.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.5_-0.33</th>\n",
       "      <td>90.32</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.86</td>\n",
       "      <td>921.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_2.0_-0.33</th>\n",
       "      <td>90.32</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>37.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_-0.25</th>\n",
       "      <td>90.24</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1816.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.25_-0.33</th>\n",
       "      <td>90.16</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1240.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_2.0_-10.0</th>\n",
       "      <td>90.16</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.76</td>\n",
       "      <td>6359.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.25_-2.0</th>\n",
       "      <td>90.08</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.88</td>\n",
       "      <td>4891.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.5_-0.33</th>\n",
       "      <td>90.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>254.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_-1.0</th>\n",
       "      <td>90.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1197.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_2.0_0</th>\n",
       "      <td>90.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>38.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_-3.0</th>\n",
       "      <td>89.92</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>6656.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_-0.5</th>\n",
       "      <td>89.84</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>523.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.5_-1.0</th>\n",
       "      <td>89.84</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>3025.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.25_-0.5</th>\n",
       "      <td>89.84</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1900.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_2.0_-0.5</th>\n",
       "      <td>89.76</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>35.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.33_-5.0</th>\n",
       "      <td>89.60</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6272.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_-5.0</th>\n",
       "      <td>89.60</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.82</td>\n",
       "      <td>6244.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_-1.5</th>\n",
       "      <td>89.60</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2820.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.5_-3.0</th>\n",
       "      <td>89.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4524.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.25_-5.0</th>\n",
       "      <td>89.44</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6369.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.5_-5.0</th>\n",
       "      <td>89.44</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6241.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_3.0_0</th>\n",
       "      <td>89.44</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>82.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_-2.0</th>\n",
       "      <td>89.36</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5659.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0_-0.33</th>\n",
       "      <td>89.36</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.87</td>\n",
       "      <td>885.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.5_-5.0</th>\n",
       "      <td>89.20</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>6121.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_-3.0</th>\n",
       "      <td>89.12</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5573.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.5_-0.25</th>\n",
       "      <td>88.88</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>226.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.5_-1.5</th>\n",
       "      <td>88.88</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>92.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.33_-0.5</th>\n",
       "      <td>88.80</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>215.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_10.0_-20.0</th>\n",
       "      <td>88.48</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.72</td>\n",
       "      <td>6467.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_-5.0</th>\n",
       "      <td>88.40</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.84</td>\n",
       "      <td>6095.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.33_-1.5</th>\n",
       "      <td>88.32</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6426.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_2.0_-5.0</th>\n",
       "      <td>88.32</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.86</td>\n",
       "      <td>6449.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.5_-2.0</th>\n",
       "      <td>88.16</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2533.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.5_-0.5</th>\n",
       "      <td>88.00</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>285.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_3.0_-10.0</th>\n",
       "      <td>87.68</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.78</td>\n",
       "      <td>6680.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.25_-1.0</th>\n",
       "      <td>87.68</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.85</td>\n",
       "      <td>6048.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_10.0_-10.0</th>\n",
       "      <td>87.68</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2513.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0_-0.25</th>\n",
       "      <td>87.52</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>452.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_2.0_-2.0</th>\n",
       "      <td>87.52</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>132.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_2.0_0</th>\n",
       "      <td>87.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>168.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.33_-1.0</th>\n",
       "      <td>87.44</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3390.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.25_-0.5</th>\n",
       "      <td>87.28</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>486.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.5_-1.5</th>\n",
       "      <td>87.28</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1316.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_5.0_-5.0</th>\n",
       "      <td>87.04</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2090.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.5_-1.0</th>\n",
       "      <td>86.96</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>555.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_5.0_-0.33</th>\n",
       "      <td>86.88</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>45.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_3.0_-0.25</th>\n",
       "      <td>86.80</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>101.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_3.0_0</th>\n",
       "      <td>86.80</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>47.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_5.0_0</th>\n",
       "      <td>86.64</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>45.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_3.0_-0.33</th>\n",
       "      <td>86.48</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>45.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_3.0_-0.33</th>\n",
       "      <td>86.48</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>105.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_5.0_-0.25</th>\n",
       "      <td>86.32</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>45.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_5.0_-0.5</th>\n",
       "      <td>86.24</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>47.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_3.0_-0.5</th>\n",
       "      <td>86.24</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>45.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_3.0_-1.0</th>\n",
       "      <td>86.08</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>42.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_2.0_-2.0</th>\n",
       "      <td>86.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1368.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_3.0_-0.25</th>\n",
       "      <td>85.92</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>45.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_3.0_-0.5</th>\n",
       "      <td>85.68</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>105.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_2.0_-0.5</th>\n",
       "      <td>85.60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>235.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_2.0_-0.25</th>\n",
       "      <td>85.60</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>251.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_3.0_-5.0</th>\n",
       "      <td>85.60</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5518.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_3.0_-3.0</th>\n",
       "      <td>85.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1577.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_20.0_-20.0</th>\n",
       "      <td>85.36</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.81</td>\n",
       "      <td>3868.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_3.0_-1.5</th>\n",
       "      <td>85.20</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>39.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_2.0_-1.0</th>\n",
       "      <td>85.04</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>324.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.25_-1.0</th>\n",
       "      <td>85.04</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3845.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_2.0_-0.33</th>\n",
       "      <td>84.88</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>229.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_10.0_-0.33</th>\n",
       "      <td>84.16</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>52.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_2.0_-1.5</th>\n",
       "      <td>84.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>610.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_3.0_-2.0</th>\n",
       "      <td>84.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>39.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_10.0_0</th>\n",
       "      <td>83.84</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>53.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_10.0_-0.25</th>\n",
       "      <td>83.84</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>52.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.33_-20.0</th>\n",
       "      <td>83.76</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7319.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_10.0_-0.5</th>\n",
       "      <td>83.76</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>52.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.25_-20.0</th>\n",
       "      <td>83.44</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7369.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_5.0_-10.0</th>\n",
       "      <td>83.44</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.82</td>\n",
       "      <td>6759.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0_-20.0</th>\n",
       "      <td>83.44</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7376.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.5_-20.0</th>\n",
       "      <td>83.44</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7181.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_2.0_-20.0</th>\n",
       "      <td>83.20</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7040.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_3.0_-20.0</th>\n",
       "      <td>82.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7144.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_5.0_-1.0</th>\n",
       "      <td>82.80</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>47.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.5_-20.0</th>\n",
       "      <td>82.72</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.81</td>\n",
       "      <td>6958.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_3.0_-1.0</th>\n",
       "      <td>82.64</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>181.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_alphas_1_3.2_-3.2</th>\n",
       "      <td>82.48</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.26</td>\n",
       "      <td>8.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.0_-20.0</th>\n",
       "      <td>82.40</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7070.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_5.0_-0.33</th>\n",
       "      <td>82.32</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>64.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_5.0_0</th>\n",
       "      <td>82.32</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>69.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_10.0_-1.0</th>\n",
       "      <td>82.24</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>50.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_5.0_-0.25</th>\n",
       "      <td>82.16</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>65.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_5.0_-0.5</th>\n",
       "      <td>82.00</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>62.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_160.0_0</th>\n",
       "      <td>81.60</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>529.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_5.0_-20.0</th>\n",
       "      <td>81.44</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.80</td>\n",
       "      <td>6959.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_160.0_-3.0</th>\n",
       "      <td>81.44</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>519.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_160.0_-2.0</th>\n",
       "      <td>81.44</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>521.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_10.0_-2.0</th>\n",
       "      <td>81.44</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>48.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.5_-5.0</th>\n",
       "      <td>81.36</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5289.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_10.0_0</th>\n",
       "      <td>81.36</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>156.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_40.0_0</th>\n",
       "      <td>81.36</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>179.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_10.0_-0.33</th>\n",
       "      <td>81.28</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>149.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_160.0_-0.25</th>\n",
       "      <td>81.28</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>523.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_20.0_-0.25</th>\n",
       "      <td>81.28</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>83.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_20.0_-0.33</th>\n",
       "      <td>81.28</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>82.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.0_-5.0</th>\n",
       "      <td>81.28</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5693.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_5.0_-10.0</th>\n",
       "      <td>81.20</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5925.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_10.0_-1.5</th>\n",
       "      <td>81.20</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>124.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_160.0_-0.33</th>\n",
       "      <td>81.20</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>530.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_160.0_-0.5</th>\n",
       "      <td>81.20</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>527.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0_-5.0</th>\n",
       "      <td>81.12</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>6176.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.0_-2.0</th>\n",
       "      <td>81.12</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4817.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_160.0_-1.0</th>\n",
       "      <td>81.12</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>523.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_20.0_-0.5</th>\n",
       "      <td>81.12</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>82.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_10.0_-1.0</th>\n",
       "      <td>81.12</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>135.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_160.0_-1.5</th>\n",
       "      <td>81.04</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>522.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_40.0_-0.25</th>\n",
       "      <td>81.04</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>177.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_160.0_-5.0</th>\n",
       "      <td>81.04</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>517.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_10.0_-0.5</th>\n",
       "      <td>81.04</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>147.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_20.0_-1.0</th>\n",
       "      <td>81.04</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>80.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_20.0_0</th>\n",
       "      <td>81.04</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>83.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_10.0_-0.25</th>\n",
       "      <td>81.04</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>150.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.5_-5.0</th>\n",
       "      <td>80.96</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5866.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_40.0_-0.5</th>\n",
       "      <td>80.88</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>177.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_40.0_-0.33</th>\n",
       "      <td>80.88</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>177.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_20.0_-0.33</th>\n",
       "      <td>80.80</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.86</td>\n",
       "      <td>670.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_20.0_-0.25</th>\n",
       "      <td>80.80</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.86</td>\n",
       "      <td>669.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_20.0_-1.5</th>\n",
       "      <td>80.72</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>79.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_20.0_0</th>\n",
       "      <td>80.72</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.86</td>\n",
       "      <td>678.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_10.0_-1.5</th>\n",
       "      <td>80.72</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>50.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_-1.0</th>\n",
       "      <td>80.72</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4578.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_5.0_-1.5</th>\n",
       "      <td>80.64</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>52.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_40.0_-2.0</th>\n",
       "      <td>80.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>180.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_40.0_-1.5</th>\n",
       "      <td>80.56</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>182.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.25_-5.0</th>\n",
       "      <td>80.56</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5972.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_10.0_-2.0</th>\n",
       "      <td>80.48</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>115.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_20.0_-0.5</th>\n",
       "      <td>80.48</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.86</td>\n",
       "      <td>660.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_3.0_-3.0</th>\n",
       "      <td>80.48</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.87</td>\n",
       "      <td>304.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_160.0_-10.0</th>\n",
       "      <td>80.40</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>519.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_20.0_-2.0</th>\n",
       "      <td>80.40</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>80.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_40.0_-1.0</th>\n",
       "      <td>80.32</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>177.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_20.0_-1.0</th>\n",
       "      <td>80.32</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.86</td>\n",
       "      <td>651.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.25_-1.5</th>\n",
       "      <td>80.24</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5172.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0.5_-1.5</th>\n",
       "      <td>80.24</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5373.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_5.0_-5.0</th>\n",
       "      <td>80.16</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1060.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.33_-5.0</th>\n",
       "      <td>80.16</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>6001.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_80.0_0</th>\n",
       "      <td>80.16</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>344.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_2.0_-3.0</th>\n",
       "      <td>80.08</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5145.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_80.0_-0.25</th>\n",
       "      <td>80.08</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>347.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_80.0_-0.5</th>\n",
       "      <td>80.08</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>347.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_80.0_-1.5</th>\n",
       "      <td>80.08</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>344.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_320.0_-1.5</th>\n",
       "      <td>80.00</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>710.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_80.0_-0.33</th>\n",
       "      <td>80.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>348.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_80.0_-1.0</th>\n",
       "      <td>80.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>344.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_80.0_-2.0</th>\n",
       "      <td>80.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>342.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_320.0_-1.0</th>\n",
       "      <td>79.92</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>709.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_320.0_-2.0</th>\n",
       "      <td>79.92</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>709.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_320.0_-0.5</th>\n",
       "      <td>79.92</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>710.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.33_-1.5</th>\n",
       "      <td>79.84</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4383.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_320.0_-0.33</th>\n",
       "      <td>79.84</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>711.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_320.0_0</th>\n",
       "      <td>79.84</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>708.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_320.0_-0.25</th>\n",
       "      <td>79.84</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>708.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_320.0_-3.0</th>\n",
       "      <td>79.84</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>712.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_3.0_-1.5</th>\n",
       "      <td>79.84</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>248.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_320.0_-5.0</th>\n",
       "      <td>79.76</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>707.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_10.0_-10.0</th>\n",
       "      <td>79.76</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4663.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_80.0_-5.0</th>\n",
       "      <td>79.68</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>334.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_20.0_-2.0</th>\n",
       "      <td>79.52</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.86</td>\n",
       "      <td>595.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0_-3.0</th>\n",
       "      <td>79.52</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4927.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_20.0_-1.5</th>\n",
       "      <td>79.44</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.86</td>\n",
       "      <td>640.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_80.0_-3.0</th>\n",
       "      <td>79.44</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>338.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.5_-1.5</th>\n",
       "      <td>79.36</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4837.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_40.0_-3.0</th>\n",
       "      <td>79.28</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>182.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_3.0_-2.0</th>\n",
       "      <td>79.20</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>342.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_320.0_-10.0</th>\n",
       "      <td>79.12</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>712.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_2.0_-5.0</th>\n",
       "      <td>79.04</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "      <td>4941.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_5.0_-2.0</th>\n",
       "      <td>79.04</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>90.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_5.0_-1.0</th>\n",
       "      <td>78.96</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>60.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_20.0_-3.0</th>\n",
       "      <td>78.88</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>77.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_320.0_-20.0</th>\n",
       "      <td>78.80</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>688.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_40.0_-5.0</th>\n",
       "      <td>78.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>174.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0_-10.0</th>\n",
       "      <td>78.56</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7091.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0_-1.0</th>\n",
       "      <td>78.48</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4912.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_20.0_-3.0</th>\n",
       "      <td>78.48</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.86</td>\n",
       "      <td>492.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_160.0_-20.0</th>\n",
       "      <td>78.24</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.85</td>\n",
       "      <td>518.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.33_-10.0</th>\n",
       "      <td>78.16</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7000.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_40.0_0</th>\n",
       "      <td>78.16</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2508.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_40.0_-0.25</th>\n",
       "      <td>78.08</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2508.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_40.0_-2.0</th>\n",
       "      <td>78.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2377.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.5_-10.0</th>\n",
       "      <td>77.92</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7015.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_40.0_-0.5</th>\n",
       "      <td>77.92</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2508.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_40.0_-0.33</th>\n",
       "      <td>77.92</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2508.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_5.0_-1.5</th>\n",
       "      <td>77.92</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>56.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.25_-10.0</th>\n",
       "      <td>77.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7046.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_40.0_-1.5</th>\n",
       "      <td>77.84</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2446.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.25_-3.0</th>\n",
       "      <td>77.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5060.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_20.0_-5.0</th>\n",
       "      <td>77.44</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>70.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_80.0_-10.0</th>\n",
       "      <td>77.36</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>309.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_40.0_-1.0</th>\n",
       "      <td>77.36</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2463.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.5_-2.0</th>\n",
       "      <td>77.36</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>5152.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0_-1.5</th>\n",
       "      <td>77.28</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>5247.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_40.0_-3.0</th>\n",
       "      <td>77.28</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2321.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_-2.0</th>\n",
       "      <td>77.20</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5578.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_5.0_-2.0</th>\n",
       "      <td>77.12</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>52.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.33_-3.0</th>\n",
       "      <td>77.12</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5215.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_10.0_-3.0</th>\n",
       "      <td>77.12</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>49.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.0_-10.0</th>\n",
       "      <td>77.04</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7221.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_40.0_-5.0</th>\n",
       "      <td>76.96</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2211.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_10.0_-3.0</th>\n",
       "      <td>76.80</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>100.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_80.0_0</th>\n",
       "      <td>76.56</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4287.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_80.0_-10.0</th>\n",
       "      <td>76.56</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3937.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_80.0_-0.25</th>\n",
       "      <td>76.48</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4285.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.5_-10.0</th>\n",
       "      <td>76.40</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7236.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_3.0_-10.0</th>\n",
       "      <td>76.40</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6829.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_80.0_-0.33</th>\n",
       "      <td>76.40</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4285.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_80.0_-0.5</th>\n",
       "      <td>76.24</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4273.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_2.0_-3.0</th>\n",
       "      <td>76.24</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "      <td>6001.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_80.0_-1.0</th>\n",
       "      <td>76.16</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4224.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_20.0_-5.0</th>\n",
       "      <td>76.16</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>357.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_80.0_-1.5</th>\n",
       "      <td>76.16</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4177.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_80.0_-2.0</th>\n",
       "      <td>76.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4171.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_80.0_-5.0</th>\n",
       "      <td>75.92</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4068.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_10.0_-20.0</th>\n",
       "      <td>75.76</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.80</td>\n",
       "      <td>7181.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_40.0_-10.0</th>\n",
       "      <td>75.68</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2071.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_2.0_-10.0</th>\n",
       "      <td>75.60</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6930.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_40.0_-10.0</th>\n",
       "      <td>75.52</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>148.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_80.0_-3.0</th>\n",
       "      <td>75.52</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4161.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_5.0_-3.0</th>\n",
       "      <td>74.88</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>50.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.5_-3.0</th>\n",
       "      <td>74.80</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5312.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_80.0_-20.0</th>\n",
       "      <td>74.72</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>277.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_80.0_-20.0</th>\n",
       "      <td>74.56</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3642.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0_-2.0</th>\n",
       "      <td>74.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.88</td>\n",
       "      <td>4764.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_160.0_0</th>\n",
       "      <td>74.24</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5189.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_160.0_-0.33</th>\n",
       "      <td>74.16</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5181.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_160.0_-0.25</th>\n",
       "      <td>74.16</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5183.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_160.0_-1.0</th>\n",
       "      <td>74.08</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5177.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_160.0_-3.0</th>\n",
       "      <td>74.08</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5158.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_160.0_-2.0</th>\n",
       "      <td>74.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5176.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_320.0_-5.0</th>\n",
       "      <td>74.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5637.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_160.0_-1.5</th>\n",
       "      <td>74.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5179.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_320.0_-10.0</th>\n",
       "      <td>73.92</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5626.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_320.0_0</th>\n",
       "      <td>73.92</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5624.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_160.0_-0.5</th>\n",
       "      <td>73.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5171.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_320.0_-0.5</th>\n",
       "      <td>73.92</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5639.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_320.0_-0.33</th>\n",
       "      <td>73.92</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5637.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_320.0_-1.5</th>\n",
       "      <td>73.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5649.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_320.0_-0.25</th>\n",
       "      <td>73.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5638.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_160.0_-20.0</th>\n",
       "      <td>73.84</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5121.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_320.0_-1.0</th>\n",
       "      <td>73.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5640.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_160.0_-5.0</th>\n",
       "      <td>73.84</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5161.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_320.0_-20.0</th>\n",
       "      <td>73.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5629.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_320.0_-3.0</th>\n",
       "      <td>73.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5645.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_160.0_-10.0</th>\n",
       "      <td>73.76</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5175.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.0_-3.0</th>\n",
       "      <td>73.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5112.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_320.0_-2.0</th>\n",
       "      <td>73.68</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5641.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_40.0_-20.0</th>\n",
       "      <td>73.52</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.86</td>\n",
       "      <td>933.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_1.5_-3.0</th>\n",
       "      <td>73.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>5217.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_3.0_-5.0</th>\n",
       "      <td>73.20</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5843.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.33_-2.0</th>\n",
       "      <td>73.12</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4978.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_0.25_-2.0</th>\n",
       "      <td>72.56</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4935.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_20.0_-10.0</th>\n",
       "      <td>71.12</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>185.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer12_alphas_1.0_10.0_-5.0</th>\n",
       "      <td>69.68</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>78.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_10.0_-5.0</th>\n",
       "      <td>69.04</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>50.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_5.0_-3.0</th>\n",
       "      <td>67.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>246.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_20.0_-10.0</th>\n",
       "      <td>65.04</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>62.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_40.0_-20.0</th>\n",
       "      <td>63.92</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>112.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_20.0_-20.0</th>\n",
       "      <td>59.84</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5629.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_alphas_1_3.2_-3.2 (on full data)</th>\n",
       "      <td>33.92</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>47.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               pos_prompts  dist-1  dist-2  \\\n",
       "dexperts_steer_layer12_alphas_1.0_0.25_0             94.80    0.58    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_0             94.72    0.58    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_0              94.72    0.58    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-0.33          94.40    0.58    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-0.25          94.08    0.58    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-0.25         93.76    0.58    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-0.33          93.52    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-20.0          93.20    0.78    0.84   \n",
       "dexperts_steer_layer12_alphas_1.0_0_0                93.20    0.58    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_0             93.12    0.53    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-20.0          93.12    0.78    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-20.0          93.12    0.78    0.84   \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-0.5           93.12    0.58    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-0.25          93.04    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-10.0         92.88    0.73    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-20.0          92.80    0.78    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-1.5           92.72    0.60    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_0_-10.0            92.64    0.73    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-10.0         92.64    0.73    0.84   \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-0.33         92.56    0.59    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-20.0          92.48    0.78    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-1.0           92.48    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_0              92.40    0.55    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-10.0          92.40    0.73    0.84   \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-1.0          92.40    0.63    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-20.0          92.40    0.78    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-0.5           92.32    0.59    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_0              92.32    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-20.0         92.24    0.78    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_0_-20.0            92.24    0.78    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-20.0         92.16    0.78    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_0_-0.5             92.08    0.60    0.89   \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-0.25         92.08    0.59    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-0.25         92.00    0.49    0.84   \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-0.5           92.00    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-2.0           91.92    0.46    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-0.25          91.92    0.58    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_0              91.84    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0_0                91.84    0.47    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-1.5          91.68    0.49    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-0.33          91.60    0.55    0.84   \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-0.33          91.52    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-3.0          91.52    0.56    0.88   \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-1.0           91.44    0.58    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_0_-1.5             91.44    0.45    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-1.5           91.44    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-0.5          91.36    0.45    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-0.25          91.36    0.52    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_0_-0.33            91.28    0.47    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-2.0           91.28    0.61    0.89   \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-0.33         91.20    0.58    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-1.0           91.12    0.61    0.89   \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-1.0           91.12    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-10.0          91.04    0.73    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_0             91.04    0.52    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-0.33         90.88    0.47    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_0              90.88    0.55    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_0              90.88    0.55    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-0.5           90.72    0.48    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-2.0          90.72    0.50    0.89   \n",
       "dexperts_steer_layer22_alphas_1.0_0_-0.5             90.72    0.50    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-0.25         90.64    0.47    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-10.0          90.56    0.73    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-0.25          90.48    0.55    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-3.0          90.40    0.61    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-3.0           90.32    0.53    0.89   \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-0.25          90.32    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-0.33          90.32    0.51    0.84   \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-0.33          90.32    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0_-0.25            90.24    0.45    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-0.33         90.16    0.46    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-10.0          90.16    0.72    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-2.0          90.08    0.52    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-0.33          90.08    0.55    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-1.0           90.00    0.48    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_0              90.00    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0_-3.0             89.92    0.66    0.91   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-0.5           89.84    0.56    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-1.0           89.84    0.49    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-0.5          89.84    0.45    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-0.5           89.76    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-5.0          89.60    0.65    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_0_-5.0             89.60    0.66    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-1.5           89.60    0.47    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-3.0           89.44    0.52    0.89   \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-5.0          89.44    0.66    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-5.0           89.44    0.65    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_0              89.44    0.57    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_0_-2.0             89.36    0.58    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_0_-0.33            89.36    0.60    0.89   \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-5.0           89.20    0.64    0.89   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-3.0           89.12    0.61    0.91   \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-0.25          88.88    0.55    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-1.5           88.88    0.59    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-0.5          88.80    0.59    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-20.0         88.48    0.78    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-5.0           88.40    0.64    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-1.5          88.32    0.54    0.92   \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-5.0           88.32    0.66    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-2.0           88.16    0.44    0.83   \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-0.5           88.00    0.55    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-10.0          87.68    0.73    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-1.0          87.68    0.69    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-10.0         87.68    0.75    0.89   \n",
       "dexperts_steer_layer12_alphas_1.0_0_-0.25            87.52    0.60    0.88   \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-2.0           87.52    0.59    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_0              87.52    0.57    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-1.0          87.44    0.52    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-0.5          87.28    0.60    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-1.5           87.28    0.48    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-5.0           87.04    0.45    0.84   \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-1.0           86.96    0.56    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-0.33          86.88    0.57    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-0.25          86.80    0.57    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_0              86.80    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_0              86.64    0.57    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-0.33          86.48    0.59    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-0.33          86.48    0.57    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-0.25          86.32    0.57    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-0.5           86.24    0.57    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-0.5           86.24    0.59    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-1.0           86.08    0.59    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-2.0           86.00    0.47    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-0.25          85.92    0.59    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-0.5           85.68    0.57    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-0.5           85.60    0.55    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-0.25          85.60    0.56    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-5.0           85.60    0.63    0.91   \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-3.0           85.44    0.44    0.84   \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-20.0         85.36    0.68    0.88   \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-1.5           85.20    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-1.0           85.04    0.55    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-1.0          85.04    0.54    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-0.33          84.88    0.55    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-0.33         84.16    0.59    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-1.5           84.00    0.56    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-2.0           84.00    0.58    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_0             83.84    0.59    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-0.25         83.84    0.59    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-20.0         83.76    0.84    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-0.5          83.76    0.59    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-20.0         83.44    0.85    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-10.0          83.44    0.70    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_0_-20.0            83.44    0.84    0.90   \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-20.0          83.44    0.84    0.90   \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-20.0          83.20    0.85    0.90   \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-20.0          82.88    0.86    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-1.0           82.80    0.57    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-20.0          82.72    0.84    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-1.0           82.64    0.57    0.86   \n",
       "dexperts_alphas_1_3.2_-3.2                           82.48    0.14    0.23   \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-20.0          82.40    0.85    0.90   \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-0.33          82.32    0.59    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_0              82.32    0.60    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-1.0          82.24    0.59    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-0.25          82.16    0.59    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-0.5           82.00    0.59    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_0            81.60    0.72    0.88   \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-20.0          81.44    0.87    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-3.0         81.44    0.72    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-2.0         81.44    0.72    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-2.0          81.44    0.59    0.85   \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-5.0           81.36    0.61    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_0             81.36    0.62    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_0             81.36    0.66    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-0.33         81.28    0.61    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-0.25        81.28    0.72    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-0.25         81.28    0.62    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-0.33         81.28    0.62    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-5.0           81.28    0.66    0.93   \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-10.0          81.20    0.74    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-1.5          81.20    0.60    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-0.33        81.20    0.72    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-0.5         81.20    0.72    0.88   \n",
       "dexperts_steer_layer12_alphas_1.0_0_-5.0             81.12    0.73    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-2.0           81.12    0.55    0.89   \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-1.0         81.12    0.72    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-0.5          81.12    0.61    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-1.0          81.12    0.61    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-1.5         81.04    0.72    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-0.25         81.04    0.66    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-5.0         81.04    0.72    0.88   \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-0.5          81.04    0.61    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-1.0          81.04    0.61    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_0             81.04    0.62    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-0.25         81.04    0.61    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-5.0           80.96    0.70    0.92   \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-0.5          80.88    0.66    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-0.33         80.88    0.66    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-0.33         80.80    0.70    0.89   \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-0.25         80.80    0.70    0.89   \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-1.5          80.72    0.61    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_0             80.72    0.70    0.89   \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-1.5          80.72    0.58    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_0_-1.0             80.72    0.53    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-1.5           80.64    0.57    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-2.0          80.64    0.66    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-1.5          80.56    0.66    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-5.0          80.56    0.71    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-2.0          80.48    0.60    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-0.5          80.48    0.70    0.89   \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-3.0           80.48    0.61    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-10.0        80.40    0.72    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-2.0          80.40    0.61    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-1.0          80.32    0.66    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-1.0          80.32    0.70    0.89   \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-1.5          80.24    0.54    0.91   \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-1.5           80.24    0.55    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-5.0           80.16    0.65    0.89   \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-5.0          80.16    0.71    0.92   \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_0             80.16    0.70    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-3.0           80.08    0.59    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-0.25         80.08    0.70    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-0.5          80.08    0.70    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-1.5          80.08    0.70    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-1.5         80.00    0.74    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-0.33         80.00    0.70    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-1.0          80.00    0.70    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-2.0          80.00    0.70    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-1.0         79.92    0.74    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-2.0         79.92    0.74    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-0.5         79.92    0.74    0.88   \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-1.5          79.84    0.53    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-0.33        79.84    0.74    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_0            79.84    0.74    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-0.25        79.84    0.74    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-3.0         79.84    0.74    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-1.5           79.84    0.55    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-5.0         79.76    0.74    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-10.0         79.76    0.61    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-5.0          79.68    0.69    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-2.0          79.52    0.69    0.89   \n",
       "dexperts_steer_layer12_alphas_1.0_0_-3.0             79.52    0.59    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-1.5          79.44    0.70    0.89   \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-3.0          79.44    0.70    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-1.5           79.36    0.56    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-3.0          79.28    0.66    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-2.0           79.20    0.56    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-10.0        79.12    0.73    0.88   \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-5.0           79.04    0.58    0.92   \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-2.0           79.04    0.58    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-1.0           78.96    0.59    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-3.0          78.88    0.61    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-20.0        78.80    0.73    0.88   \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-5.0          78.64    0.65    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_0_-10.0            78.56    0.87    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_0_-1.0             78.48    0.55    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-3.0          78.48    0.68    0.89   \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-20.0        78.24    0.72    0.88   \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-10.0         78.16    0.87    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_0             78.16    0.80    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-0.25         78.08    0.80    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-2.0          78.00    0.80    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-10.0          77.92    0.87    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-0.5          77.92    0.80    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-0.33         77.92    0.80    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-1.5           77.92    0.59    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-10.0         77.84    0.87    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-1.5          77.84    0.80    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-3.0          77.52    0.56    0.92   \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-5.0          77.44    0.61    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-10.0         77.36    0.69    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-1.0          77.36    0.80    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-2.0           77.36    0.53    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_0_-1.5             77.28    0.54    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-3.0          77.28    0.79    0.91   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-2.0           77.20    0.57    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-2.0           77.12    0.59    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-3.0          77.12    0.56    0.92   \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-3.0          77.12    0.58    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-10.0          77.04    0.87    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-5.0          76.96    0.79    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-3.0          76.80    0.60    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_0             76.56    0.85    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-10.0         76.56    0.84    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-0.25         76.48    0.85    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-10.0          76.40    0.86    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-10.0          76.40    0.84    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-0.33         76.40    0.85    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-0.5          76.24    0.85    0.91   \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-3.0           76.24    0.61    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-1.0          76.16    0.85    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-5.0          76.16    0.66    0.88   \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-1.5          76.16    0.85    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-2.0          76.00    0.84    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-5.0          75.92    0.84    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-20.0         75.76    0.91    0.90   \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-10.0         75.68    0.78    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-10.0          75.60    0.85    0.91   \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-10.0         75.52    0.65    0.86   \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-3.0          75.52    0.85    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-3.0           74.88    0.59    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-3.0           74.80    0.57    0.92   \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-20.0         74.72    0.68    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-20.0         74.56    0.83    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_0_-2.0             74.48    0.52    0.90   \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_0            74.24    0.86    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-0.33        74.16    0.86    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-0.25        74.16    0.86    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-1.0         74.08    0.86    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-3.0         74.08    0.86    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-2.0         74.00    0.86    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-5.0         74.00    0.87    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-1.5         74.00    0.86    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-10.0        73.92    0.87    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_0            73.92    0.87    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-0.5         73.92    0.86    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-0.5         73.92    0.87    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-0.33        73.92    0.87    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-1.5         73.84    0.87    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-0.25        73.84    0.87    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-20.0        73.84    0.86    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-1.0         73.84    0.87    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-5.0         73.84    0.86    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-20.0        73.84    0.87    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-3.0         73.84    0.87    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-10.0        73.76    0.86    0.91   \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-3.0           73.68    0.53    0.90   \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-2.0         73.68    0.87    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-20.0         73.52    0.72    0.90   \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-3.0           73.52    0.52    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-5.0           73.20    0.57    0.93   \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-2.0          73.12    0.53    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-2.0          72.56    0.53    0.92   \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-10.0         71.12    0.63    0.87   \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-5.0          69.68    0.60    0.87   \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-5.0          69.04    0.57    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-3.0           67.52    0.57    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-10.0         65.04    0.59    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-20.0         63.92    0.62    0.86   \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-20.0         59.84    0.65    0.92   \n",
       "dexperts_alphas_1_3.2_-3.2 (on full data)            33.92    0.56    0.81   \n",
       "\n",
       "                                               dist-3  perplexity  \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_0         0.86       41.81  \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_0         0.86       36.89  \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_0          0.86       34.86  \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-0.33      0.86       43.56  \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-0.25      0.86       42.32  \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-0.25     0.86       53.89  \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-0.33      0.86       33.85  \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-20.0      0.72     6113.54  \n",
       "dexperts_steer_layer12_alphas_1.0_0_0            0.86       70.24  \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_0         0.85      668.30  \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-20.0      0.73     6164.77  \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-20.0      0.71     5956.74  \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-0.5       0.86       36.62  \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-0.25      0.86       33.07  \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-10.0     0.73     6035.95  \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-20.0      0.73     6216.48  \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-1.5       0.85     1873.29  \n",
       "dexperts_steer_layer22_alphas_1.0_0_-10.0        0.73     5922.66  \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-10.0     0.74     6036.40  \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-0.33     0.86       67.66  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-20.0      0.74     6185.63  \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-1.0       0.86       38.36  \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_0          0.85      495.97  \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-10.0      0.74     6071.51  \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-1.0      0.85     3959.14  \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-20.0      0.74     6185.30  \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-0.5       0.86       71.62  \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_0          0.86       34.38  \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-20.0     0.74     6174.56  \n",
       "dexperts_steer_layer22_alphas_1.0_0_-20.0        0.74     6097.82  \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-20.0     0.74     6241.20  \n",
       "dexperts_steer_layer12_alphas_1.0_0_-0.5         0.85     1740.90  \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-0.25     0.86       68.83  \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-0.25     0.86     1036.43  \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-0.5       0.86       33.73  \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-2.0       0.88     4209.18  \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-0.25      0.86       34.57  \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_0          0.86       35.10  \n",
       "dexperts_steer_layer22_alphas_1.0_0_0            0.87     1093.55  \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-1.5      0.90     6143.19  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-0.33      0.85      403.72  \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-0.33      0.86       34.38  \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-3.0      0.86     4655.43  \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-1.0       0.86       78.22  \n",
       "dexperts_steer_layer22_alphas_1.0_0_-1.5         0.89     4385.16  \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-1.5       0.86       41.13  \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-0.5      0.88     1522.66  \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-0.25      0.86      794.70  \n",
       "dexperts_steer_layer22_alphas_1.0_0_-0.33        0.88     2339.30  \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-2.0       0.85     2156.35  \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-0.33     0.86       99.66  \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-1.0       0.85     1803.01  \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-1.0       0.86       34.98  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-10.0      0.75     6214.46  \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_0         0.86      782.23  \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-0.33     0.87     1098.30  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_0          0.86      291.33  \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_0          0.87      225.06  \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-0.5       0.87     1116.16  \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-2.0      0.88     4608.70  \n",
       "dexperts_steer_layer22_alphas_1.0_0_-0.5         0.87     3007.84  \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-0.25     0.87     1081.66  \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-10.0      0.76     6231.63  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-0.25      0.85      356.50  \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-3.0      0.86     5319.71  \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-3.0       0.87     5537.30  \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-0.25      0.86       37.74  \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-0.33      0.86      921.17  \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-0.33      0.86       37.47  \n",
       "dexperts_steer_layer22_alphas_1.0_0_-0.25        0.88     1816.74  \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-0.33     0.87     1240.60  \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-10.0      0.76     6359.31  \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-2.0      0.88     4891.44  \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-0.33      0.86      254.77  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-1.0       0.87     1197.94  \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_0          0.86       38.34  \n",
       "dexperts_steer_layer22_alphas_1.0_0_-3.0         0.86     6656.44  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-0.5       0.85      523.20  \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-1.0       0.87     3025.68  \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-0.5      0.88     1900.27  \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-0.5       0.86       35.93  \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-5.0      0.83     6272.53  \n",
       "dexperts_steer_layer22_alphas_1.0_0_-5.0         0.82     6244.85  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-1.5       0.87     2820.90  \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-3.0       0.87     4524.52  \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-5.0      0.83     6369.09  \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-5.0       0.83     6241.95  \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_0          0.85       82.15  \n",
       "dexperts_steer_layer22_alphas_1.0_0_-2.0         0.87     5659.50  \n",
       "dexperts_steer_layer12_alphas_1.0_0_-0.33        0.87      885.47  \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-5.0       0.85     6121.70  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-3.0       0.87     5573.99  \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-0.25      0.87      226.99  \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-1.5       0.86       92.59  \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-0.5      0.86      215.86  \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-20.0     0.72     6467.67  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-5.0       0.84     6095.05  \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-1.5      0.90     6426.18  \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-5.0       0.86     6449.13  \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-2.0       0.87     2533.84  \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-0.5       0.86      285.24  \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-10.0      0.78     6680.70  \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-1.0      0.85     6048.00  \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-10.0     0.81     2513.34  \n",
       "dexperts_steer_layer12_alphas_1.0_0_-0.25        0.86      452.36  \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-2.0       0.86      132.26  \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_0          0.86      168.22  \n",
       "dexperts_steer_layer22_alphas_1.0_0.33_-1.0      0.86     3390.57  \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-0.5      0.86      486.87  \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-1.5       0.87     1316.45  \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-5.0       0.87     2090.87  \n",
       "dexperts_steer_layer22_alphas_1.0_1.5_-1.0       0.85      555.79  \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-0.33      0.86       45.42  \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-0.25      0.86      101.55  \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_0          0.86       47.55  \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_0          0.86       45.13  \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-0.33      0.86       45.53  \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-0.33      0.86      105.32  \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-0.25      0.86       45.14  \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-0.5       0.86       47.44  \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-0.5       0.86       45.06  \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-1.0       0.86       42.41  \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-2.0       0.87     1368.61  \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-0.25      0.86       45.72  \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-0.5       0.86      105.08  \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-0.5       0.87      235.27  \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-0.25      0.86      251.03  \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-5.0       0.87     5518.12  \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-3.0       0.87     1577.73  \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-20.0     0.81     3868.93  \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-1.5       0.86       39.62  \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-1.0       0.86      324.23  \n",
       "dexperts_steer_layer22_alphas_1.0_0.25_-1.0      0.86     3845.90  \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-0.33      0.86      229.63  \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-0.33     0.85       52.28  \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-1.5       0.85      610.20  \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-2.0       0.86       39.91  \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_0         0.85       53.13  \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-0.25     0.85       52.53  \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-20.0     0.82     7319.58  \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-0.5      0.85       52.25  \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-20.0     0.82     7369.20  \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-10.0      0.82     6759.75  \n",
       "dexperts_steer_layer12_alphas_1.0_0_-20.0        0.82     7376.11  \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-20.0      0.82     7181.28  \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-20.0      0.81     7040.32  \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-20.0      0.81     7144.14  \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-1.0       0.86       47.75  \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-20.0      0.81     6958.85  \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-1.0       0.86      181.81  \n",
       "dexperts_alphas_1_3.2_-3.2                       0.26        8.30  \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-20.0      0.81     7070.75  \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-0.33      0.86       64.55  \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_0          0.86       69.22  \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-1.0      0.85       50.61  \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-0.25      0.86       65.70  \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-0.5       0.86       62.73  \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_0        0.85      529.30  \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-20.0      0.80     6959.14  \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-3.0     0.85      519.22  \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-2.0     0.85      521.17  \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-2.0      0.85       48.92  \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-5.0       0.87     5289.22  \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_0         0.86      156.81  \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_0         0.85      179.93  \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-0.33     0.86      149.02  \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-0.25    0.85      523.32  \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-0.25     0.85       83.32  \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-0.33     0.85       82.69  \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-5.0       0.87     5693.68  \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-10.0      0.86     5925.30  \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-1.5      0.87      124.04  \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-0.33    0.85      530.60  \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-0.5     0.85      527.69  \n",
       "dexperts_steer_layer12_alphas_1.0_0_-5.0         0.86     6176.79  \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-2.0       0.86     4817.33  \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-1.0     0.85      523.60  \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-0.5      0.85       82.88  \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-1.0      0.87      135.99  \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-1.5     0.85      522.62  \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-0.25     0.85      177.49  \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-5.0     0.85      517.95  \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-0.5      0.86      147.24  \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-1.0      0.85       80.70  \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_0         0.85       83.82  \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-0.25     0.86      150.45  \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-5.0       0.86     5866.72  \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-0.5      0.85      177.22  \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-0.33     0.85      177.43  \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-0.33     0.86      670.74  \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-0.25     0.86      669.15  \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-1.5      0.85       79.87  \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_0         0.86      678.49  \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-1.5      0.85       50.16  \n",
       "dexperts_steer_layer22_alphas_1.0_0_-1.0         0.87     4578.59  \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-1.5       0.86       52.79  \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-2.0      0.85      180.99  \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-1.5      0.85      182.90  \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-5.0      0.86     5972.38  \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-2.0      0.86      115.91  \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-0.5      0.86      660.47  \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-3.0       0.87      304.44  \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-10.0    0.85      519.78  \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-2.0      0.85       80.12  \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-1.0      0.85      177.65  \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-1.0      0.86      651.95  \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-1.5      0.88     5172.18  \n",
       "dexperts_steer_layer22_alphas_1.0_0.5_-1.5       0.88     5373.37  \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-5.0       0.85     1060.76  \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-5.0      0.86     6001.66  \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_0         0.85      344.03  \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-3.0       0.86     5145.65  \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-0.25     0.85      347.25  \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-0.5      0.85      347.89  \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-1.5      0.85      344.24  \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-1.5     0.85      710.14  \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-0.33     0.85      348.02  \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-1.0      0.85      344.93  \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-2.0      0.85      342.80  \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-1.0     0.85      709.40  \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-2.0     0.85      709.26  \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-0.5     0.85      710.84  \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-1.5      0.87     4383.50  \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-0.33    0.85      711.10  \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_0        0.85      708.88  \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-0.25    0.85      708.92  \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-3.0     0.85      712.16  \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-1.5       0.86      248.42  \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-5.0     0.85      707.61  \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-10.0     0.86     4663.95  \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-5.0      0.85      334.60  \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-2.0      0.86      595.86  \n",
       "dexperts_steer_layer12_alphas_1.0_0_-3.0         0.87     4927.70  \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-1.5      0.86      640.54  \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-3.0      0.85      338.54  \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-1.5       0.86     4837.80  \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-3.0      0.85      182.91  \n",
       "dexperts_steer_layer22_alphas_1.0_3.0_-2.0       0.86      342.30  \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-10.0    0.85      712.06  \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-5.0       0.88     4941.58  \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-2.0       0.86       90.85  \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-1.0       0.86       60.26  \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-3.0      0.85       77.04  \n",
       "dexperts_steer_layer22_alphas_1.0_320.0_-20.0    0.85      688.05  \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-5.0      0.85      174.66  \n",
       "dexperts_steer_layer12_alphas_1.0_0_-10.0        0.82     7091.79  \n",
       "dexperts_steer_layer12_alphas_1.0_0_-1.0         0.87     4912.37  \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-3.0      0.86      492.80  \n",
       "dexperts_steer_layer22_alphas_1.0_160.0_-20.0    0.85      518.11  \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-10.0     0.82     7000.86  \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_0         0.86     2508.00  \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-0.25     0.86     2508.05  \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-2.0      0.86     2377.61  \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-10.0      0.82     7015.68  \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-0.5      0.86     2508.74  \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-0.33     0.86     2508.68  \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-1.5       0.86       56.01  \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-10.0     0.82     7046.08  \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-1.5      0.86     2446.98  \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-3.0      0.88     5060.96  \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-5.0      0.85       70.55  \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-10.0     0.85      309.17  \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-1.0      0.86     2463.61  \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-2.0       0.89     5152.02  \n",
       "dexperts_steer_layer12_alphas_1.0_0_-1.5         0.89     5247.02  \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-3.0      0.86     2321.41  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-2.0       0.88     5578.63  \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-2.0       0.86       52.26  \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-3.0      0.88     5215.59  \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-3.0      0.86       49.43  \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-10.0      0.82     7221.01  \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-5.0      0.86     2211.33  \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-3.0      0.87      100.57  \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_0         0.86     4287.58  \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-10.0     0.86     3937.12  \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-0.25     0.86     4285.59  \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-10.0      0.82     7236.95  \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-10.0      0.83     6829.87  \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-0.33     0.86     4285.87  \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-0.5      0.86     4273.22  \n",
       "dexperts_steer_layer22_alphas_1.0_2.0_-3.0       0.88     6001.98  \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-1.0      0.86     4224.17  \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-5.0      0.86      357.47  \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-1.5      0.86     4177.74  \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-2.0      0.86     4171.11  \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-5.0      0.86     4068.14  \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-20.0     0.80     7181.28  \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-10.0     0.86     2071.23  \n",
       "dexperts_steer_layer12_alphas_1.0_2.0_-10.0      0.83     6930.45  \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-10.0     0.85      148.80  \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-3.0      0.86     4161.11  \n",
       "dexperts_steer_layer12_alphas_1.0_5.0_-3.0       0.86       50.05  \n",
       "dexperts_steer_layer12_alphas_1.0_0.5_-3.0       0.88     5312.18  \n",
       "dexperts_steer_layer22_alphas_1.0_80.0_-20.0     0.85      277.25  \n",
       "dexperts_steer_layer12_alphas_1.0_80.0_-20.0     0.86     3642.18  \n",
       "dexperts_steer_layer12_alphas_1.0_0_-2.0         0.88     4764.55  \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_0        0.86     5189.00  \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-0.33    0.86     5181.37  \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-0.25    0.86     5183.47  \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-1.0     0.86     5177.10  \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-3.0     0.86     5158.81  \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-2.0     0.86     5176.99  \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-5.0     0.86     5637.56  \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-1.5     0.86     5179.95  \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-10.0    0.86     5626.02  \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_0        0.86     5624.03  \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-0.5     0.86     5171.50  \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-0.5     0.86     5639.39  \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-0.33    0.86     5637.36  \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-1.5     0.86     5649.34  \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-0.25    0.86     5638.06  \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-20.0    0.86     5121.46  \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-1.0     0.86     5640.62  \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-5.0     0.86     5161.80  \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-20.0    0.86     5629.84  \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-3.0     0.86     5645.59  \n",
       "dexperts_steer_layer12_alphas_1.0_160.0_-10.0    0.86     5175.00  \n",
       "dexperts_steer_layer12_alphas_1.0_1.0_-3.0       0.88     5112.03  \n",
       "dexperts_steer_layer12_alphas_1.0_320.0_-2.0     0.86     5641.95  \n",
       "dexperts_steer_layer12_alphas_1.0_40.0_-20.0     0.86      933.82  \n",
       "dexperts_steer_layer12_alphas_1.0_1.5_-3.0       0.89     5217.42  \n",
       "dexperts_steer_layer12_alphas_1.0_3.0_-5.0       0.88     5843.94  \n",
       "dexperts_steer_layer12_alphas_1.0_0.33_-2.0      0.89     4978.21  \n",
       "dexperts_steer_layer12_alphas_1.0_0.25_-2.0      0.89     4935.50  \n",
       "dexperts_steer_layer12_alphas_1.0_20.0_-10.0     0.86      185.07  \n",
       "dexperts_steer_layer12_alphas_1.0_10.0_-5.0      0.86       78.30  \n",
       "dexperts_steer_layer22_alphas_1.0_10.0_-5.0      0.86       50.28  \n",
       "dexperts_steer_layer22_alphas_1.0_5.0_-3.0       0.86      246.24  \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-10.0     0.86       62.69  \n",
       "dexperts_steer_layer22_alphas_1.0_40.0_-20.0     0.85      112.10  \n",
       "dexperts_steer_layer22_alphas_1.0_20.0_-20.0     0.87     5629.83  \n",
       "dexperts_alphas_1_3.2_-3.2 (on full data)        0.81       47.51  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model in pos_prompts_res.keys():\n",
    "    #print(model)\n",
    "    negative_steering_res[model] = {\n",
    "        #'neutral_prompts': neutral_prompts_res[model]['positive_proportion']*100,\n",
    "        'pos_prompts': pos_prompts_res[model]['positive_proportion']*100,\n",
    "        'dist-1': pos_prompts_res[model]['dist-1'],\n",
    "        'dist-2': pos_prompts_res[model]['dist-2'],\n",
    "        'dist-3': pos_prompts_res[model]['dist-3'],\n",
    "        'perplexity': pos_prompts_res[model]['perplexity'],\n",
    "    }\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(pd.DataFrame(negative_steering_res).transpose().sort_values(by='pos_prompts', ascending=False).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:03<00:00,  4.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_prompts</th>\n",
       "      <th>dist-1</th>\n",
       "      <th>dist-2</th>\n",
       "      <th>dist-3</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_0</th>\n",
       "      <td>89.23</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>353.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_0</th>\n",
       "      <td>88.43</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1119.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_0_1.0_0</th>\n",
       "      <td>86.04</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1350.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_-1.0</th>\n",
       "      <td>85.64</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1201.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_0_0_-1.0</th>\n",
       "      <td>82.76</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5720.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_0_2.0_-1.0</th>\n",
       "      <td>81.74</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1774.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_0_2.0_0</th>\n",
       "      <td>81.29</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>582.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_-1.0_1.0_0</th>\n",
       "      <td>81.24</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.82</td>\n",
       "      <td>4494.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_-1.0_2.0_-1.0</th>\n",
       "      <td>78.11</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.83</td>\n",
       "      <td>4850.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_0_1.0_-1.0</th>\n",
       "      <td>77.79</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5793.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_-1.0</th>\n",
       "      <td>75.76</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4567.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_10_-5</th>\n",
       "      <td>66.44</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>59.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_20_-10</th>\n",
       "      <td>61.50</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>80.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_40_-25</th>\n",
       "      <td>49.51</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>136.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_alphas_1_3.2_-3.2 (on full data)</th>\n",
       "      <td>33.92</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>47.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             pos_prompts  dist-1  dist-2  \\\n",
       "dexperts_steer_layer22_alphas_1.0_1.0_0            89.23    0.55    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_0_0              88.43    0.47    0.84   \n",
       "dexperts_steer_layer22_alphas_0_1.0_0              86.04    0.43    0.83   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-1.0         85.64    0.48    0.85   \n",
       "dexperts_steer_layer22_alphas_0_0_-1.0             82.76    0.65    0.90   \n",
       "dexperts_steer_layer22_alphas_0_2.0_-1.0           81.74    0.46    0.85   \n",
       "dexperts_steer_layer22_alphas_0_2.0_0              81.29    0.55    0.86   \n",
       "dexperts_steer_layer22_alphas_-1.0_1.0_0           81.24    0.68    0.89   \n",
       "dexperts_steer_layer22_alphas_-1.0_2.0_-1.0        78.11    0.66    0.88   \n",
       "dexperts_steer_layer22_alphas_0_1.0_-1.0           77.79    0.62    0.90   \n",
       "dexperts_steer_layer22_alphas_1.0_0_-1.0           75.76    0.50    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_10_-5            66.44    0.58    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_20_-10           61.50    0.60    0.85   \n",
       "dexperts_steer_layer22_alphas_1.0_40_-25           49.51    0.61    0.86   \n",
       "dexperts_alphas_1_3.2_-3.2 (on full data)          33.92    0.56    0.81   \n",
       "\n",
       "                                             dist-3  perplexity  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_0        0.85      353.64  \n",
       "dexperts_steer_layer22_alphas_1.0_0_0          0.87     1119.95  \n",
       "dexperts_steer_layer22_alphas_0_1.0_0          0.87     1350.88  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-1.0     0.87     1201.02  \n",
       "dexperts_steer_layer22_alphas_0_0_-1.0         0.84     5720.54  \n",
       "dexperts_steer_layer22_alphas_0_2.0_-1.0       0.87     1774.32  \n",
       "dexperts_steer_layer22_alphas_0_2.0_0          0.86      582.89  \n",
       "dexperts_steer_layer22_alphas_-1.0_1.0_0       0.82     4494.71  \n",
       "dexperts_steer_layer22_alphas_-1.0_2.0_-1.0    0.83     4850.47  \n",
       "dexperts_steer_layer22_alphas_0_1.0_-1.0       0.86     5793.77  \n",
       "dexperts_steer_layer22_alphas_1.0_0_-1.0       0.87     4567.09  \n",
       "dexperts_steer_layer22_alphas_1.0_10_-5        0.85       59.73  \n",
       "dexperts_steer_layer22_alphas_1.0_20_-10       0.85       80.38  \n",
       "dexperts_steer_layer22_alphas_1.0_40_-25       0.85      136.11  \n",
       "dexperts_alphas_1_3.2_-3.2 (on full data)      0.81       47.51  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "POS_DIR = Path('../generations/sentiment/positive_prompts/')\n",
    "\n",
    "alpha_bases = [1.0]\n",
    "alpha_experts = [0, 0.25, 0.33, 0.5, 1.0, 1.5, 2.0, 3.0, 5.0, 10.0, 20.0, 40.0, 80.0, 160.0, 320.0]\n",
    "alpha_antiexperts = [0, -0.25, -0.33, -0.5, -1.0, -1.5, -2.0, -3.0, -5.0, -10.0, -20.0]\n",
    "layer_nums = [12, 22]\n",
    "configs = ['dexperts_steer']\n",
    "\n",
    "models = {}\n",
    "\n",
    "# baselines\n",
    "models['dexperts_alphas_1_3.2_-3.2 (on full data)'] = {\n",
    "    'pos_path': f'../generations/sentiment/positive_prompts/large_experts/negative/dexperts/alphas_1.0_3.2_-3.2/prompted_gens_dexperts.jsonl',\n",
    "}\n",
    "\n",
    "# models['dexperts_alphas_1_3.2_-3.2'] = {\n",
    "#     'pos_path': f'../generations/sentiment/positive_prompts_mini/large_experts/negative/dexperts/prompted_gens_dexperts.jsonl',\n",
    "# }\n",
    "\n",
    "#models['dexperts_antionly_alphas_1_3.2_-3.2'] = {\n",
    "#    'pos_path': POS_DIR / f'large_experts/negative/dexperts_anti_only/prompted_gens_dexperts.jsonl',\n",
    "#}\n",
    "\n",
    "# Experimental configs\n",
    "alpha_configs = [\n",
    "    (1.0, 10, -5),\n",
    "    (1.0, 20, -10),\n",
    "    (1.0, 40, -25),\n",
    "    (0, 2.0, -1.0),\n",
    "    (1.0, 0, 0),\n",
    "    (0, 1.0, 0),\n",
    "    (0, 0, -1.0),\n",
    "    (0, 1.0, -1.0),\n",
    "    (1.0, 0, -1.0),\n",
    "    (-1.0, 1.0, 0),\n",
    "    (1.0, 1.0, 0),\n",
    "    (1.0, 1.0, -1.0),\n",
    "    (-1.0, 2.0, -1.0),\n",
    "    (0, 2.0, 0),\n",
    "]\n",
    "\n",
    "for alpha_base, alpha_expert, alpha_antiexpert in alpha_configs:\n",
    "    model_name = f\"dexperts_steer_layer22_alphas_{alpha_base}_{alpha_expert}_{alpha_antiexpert}\"\n",
    "\n",
    "    models[model_name] = {\n",
    "        'pos_path': POS_DIR / f'large_experts/negative/dexperts_steer/steering_large_gpt2/alphas_{alpha_base}_{alpha_expert}_{alpha_antiexpert}/layer_22_freeze_emb_lm_head_with_expert_epochs100_train_val_sst/combine_at_layer/prompted_gens_dexperts-steer.jsonl'\n",
    "    }\n",
    "\n",
    "pos_prompts_res = read_sentiment_results({m: p['pos_path'] for m,p in models.items()}, auto_eval=True)\n",
    "    \n",
    "negative_steering_res = {}\n",
    "\n",
    "for model in pos_prompts_res.keys():\n",
    "    #print(model)\n",
    "    negative_steering_res[model] = {\n",
    "        #'neutral_prompts': neutral_prompts_res[model]['positive_proportion']*100,\n",
    "        'pos_prompts': pos_prompts_res[model]['positive_proportion']*100,\n",
    "        'dist-1': pos_prompts_res[model]['dist-1'],\n",
    "        'dist-2': pos_prompts_res[model]['dist-2'],\n",
    "        'dist-3': pos_prompts_res[model]['dist-3'],\n",
    "        'perplexity': pos_prompts_res[model]['perplexity'],\n",
    "    }\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(pd.DataFrame(negative_steering_res).transpose().sort_values(by='pos_prompts', ascending=False).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:00<00:00, 230.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_prompts</th>\n",
       "      <th>dist-1</th>\n",
       "      <th>dist-2</th>\n",
       "      <th>dist-3</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_2.0_0_0_no_norm</th>\n",
       "      <td>95.2</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>36.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_2.0_0_0_no_norm2</th>\n",
       "      <td>95.2</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>36.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_2.0_0_0_norm2</th>\n",
       "      <td>94.4</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>366.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_2.0_0_0_norm</th>\n",
       "      <td>94.4</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>366.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_2.0_0_0_debug_norm</th>\n",
       "      <td>94.4</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "      <td>366.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_0_1.0_0_debug_norm_residual</th>\n",
       "      <td>92.8</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>26.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_0_debug_norm_residual</th>\n",
       "      <td>92.8</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>27.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_0_no_norm2</th>\n",
       "      <td>92.8</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>27.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_0_debug_norm_residual</th>\n",
       "      <td>92.8</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>27.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_0_no_norm</th>\n",
       "      <td>92.8</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>27.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_2.0_0_0_debug_norm_residual</th>\n",
       "      <td>92.8</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>27.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_0_0_-1.0_debug_norm_residual</th>\n",
       "      <td>92.0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>33.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_0_norm</th>\n",
       "      <td>90.4</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1258.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_0_debug_norm</th>\n",
       "      <td>90.4</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1258.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_0_norm2</th>\n",
       "      <td>90.4</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1258.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_1.0_-1.0_debug_norm_residual</th>\n",
       "      <td>90.4</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>25.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer22_alphas_1.0_0_-1.0_debug_norm_residual</th>\n",
       "      <td>88.8</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>34.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    pos_prompts  dist-1  \\\n",
       "dexperts_steer_layer22_alphas_2.0_0_0_no_norm              95.2    0.58   \n",
       "dexperts_steer_layer22_alphas_2.0_0_0_no_norm2             95.2    0.58   \n",
       "dexperts_steer_layer22_alphas_2.0_0_0_norm2                94.4    0.56   \n",
       "dexperts_steer_layer22_alphas_2.0_0_0_norm                 94.4    0.56   \n",
       "dexperts_steer_layer22_alphas_2.0_0_0_debug_norm           94.4    0.56   \n",
       "dexperts_steer_layer22_alphas_0_1.0_0_debug_nor...         92.8    0.58   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_0_debug_n...         92.8    0.58   \n",
       "dexperts_steer_layer22_alphas_1.0_0_0_no_norm2             92.8    0.59   \n",
       "dexperts_steer_layer22_alphas_1.0_0_0_debug_nor...         92.8    0.59   \n",
       "dexperts_steer_layer22_alphas_1.0_0_0_no_norm              92.8    0.59   \n",
       "dexperts_steer_layer22_alphas_2.0_0_0_debug_nor...         92.8    0.59   \n",
       "dexperts_steer_layer22_alphas_0_0_-1.0_debug_no...         92.0    0.59   \n",
       "dexperts_steer_layer22_alphas_1.0_0_0_norm                 90.4    0.48   \n",
       "dexperts_steer_layer22_alphas_1.0_0_0_debug_norm           90.4    0.48   \n",
       "dexperts_steer_layer22_alphas_1.0_0_0_norm2                90.4    0.48   \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-1.0_debu...         90.4    0.58   \n",
       "dexperts_steer_layer22_alphas_1.0_0_-1.0_debug_...         88.8    0.58   \n",
       "\n",
       "                                                    dist-2  dist-3  perplexity  \n",
       "dexperts_steer_layer22_alphas_2.0_0_0_no_norm         0.85    0.86       36.32  \n",
       "dexperts_steer_layer22_alphas_2.0_0_0_no_norm2        0.85    0.86       36.32  \n",
       "dexperts_steer_layer22_alphas_2.0_0_0_norm2           0.85    0.87      366.11  \n",
       "dexperts_steer_layer22_alphas_2.0_0_0_norm            0.85    0.87      366.11  \n",
       "dexperts_steer_layer22_alphas_2.0_0_0_debug_norm      0.85    0.87      366.11  \n",
       "dexperts_steer_layer22_alphas_0_1.0_0_debug_nor...    0.85    0.86       26.04  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_0_debug_n...    0.85    0.86       27.43  \n",
       "dexperts_steer_layer22_alphas_1.0_0_0_no_norm2        0.84    0.85       27.55  \n",
       "dexperts_steer_layer22_alphas_1.0_0_0_debug_nor...    0.84    0.85       27.55  \n",
       "dexperts_steer_layer22_alphas_1.0_0_0_no_norm         0.84    0.85       27.55  \n",
       "dexperts_steer_layer22_alphas_2.0_0_0_debug_nor...    0.84    0.85       27.55  \n",
       "dexperts_steer_layer22_alphas_0_0_-1.0_debug_no...    0.85    0.86       33.30  \n",
       "dexperts_steer_layer22_alphas_1.0_0_0_norm            0.86    0.88     1258.26  \n",
       "dexperts_steer_layer22_alphas_1.0_0_0_debug_norm      0.86    0.88     1258.26  \n",
       "dexperts_steer_layer22_alphas_1.0_0_0_norm2           0.86    0.88     1258.26  \n",
       "dexperts_steer_layer22_alphas_1.0_1.0_-1.0_debu...    0.85    0.86       25.84  \n",
       "dexperts_steer_layer22_alphas_1.0_0_-1.0_debug_...    0.85    0.86       34.64  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "POS_DIR = Path('../generations/sentiment/positive_prompts_test/')\n",
    "\n",
    "# alpha_bases = [1.0]\n",
    "# alpha_experts = [0, 0.25, 0.33, 0.5, 1.0, 1.5, 2.0, 3.0, 5.0, 10.0, 20.0, 40.0, 80.0, 160.0, 320.0]\n",
    "# alpha_antiexperts = [0, -0.25, -0.33, -0.5, -1.0, -1.5, -2.0, -3.0, -5.0, -10.0, -20.0]\n",
    "layer_nums = [22]\n",
    "configs = ['dexperts_steer']\n",
    "\n",
    "models = {}\n",
    "\n",
    "# baselines\n",
    "# models['dexperts_alphas_1_3.2_-3.2 (on full data)'] = {\n",
    "#     'pos_path': f'../generations/sentiment/positive_prompts/large_experts/negative/dexperts/alphas_1.0_3.2_-3.2/prompted_gens_dexperts.jsonl',\n",
    "# }\n",
    "\n",
    "# models['dexperts_alphas_1_3.2_-3.2'] = {\n",
    "#     'pos_path': f'../generations/sentiment/positive_prompts_mini/large_experts/negative/dexperts/prompted_gens_dexperts.jsonl',\n",
    "# }\n",
    "\n",
    "#models['dexperts_antionly_alphas_1_3.2_-3.2'] = {\n",
    "#    'pos_path': POS_DIR / f'large_experts/negative/dexperts_anti_only/prompted_gens_dexperts.jsonl',\n",
    "#}\n",
    "\n",
    "# Experimental configs\n",
    "alpha_configs = [\n",
    "    (1.0, 0, 0),\n",
    "    (2.0, 0, 0),\n",
    "]\n",
    "\n",
    "for alpha_base, alpha_expert, alpha_antiexpert in alpha_configs:\n",
    "    for exp_config in ['norm', 'no_norm', 'norm2', 'no_norm2', 'debug_norm', 'debug_norm_residual']:\n",
    "        model_name = f\"dexperts_steer_layer22_alphas_{alpha_base}_{alpha_expert}_{alpha_antiexpert}_{exp_config}\"\n",
    "\n",
    "        models[model_name] = {\n",
    "            'pos_path': POS_DIR / f'large_experts/negative/{exp_config}/dexperts_steer/steering_large_gpt2/alphas_{alpha_base}_{alpha_expert}_{alpha_antiexpert}/layer_22_freeze_emb_lm_head_with_expert_epochs100_train_val_sst/combine_at_layer/prompted_gens_dexperts-steer.jsonl'\n",
    "        }\n",
    "\n",
    "        \n",
    "more_configs = [\n",
    "    (0, 1.0, 0),\n",
    "    (0, 0, -1.0),\n",
    "    (1.0, 1.0, 0),\n",
    "    (1.0, 0, -1.0),\n",
    "    (1.0, 1.0, -1.0),\n",
    "]\n",
    "\n",
    "for alpha_base, alpha_expert, alpha_antiexpert in more_configs:\n",
    "    for exp_config in ['debug_norm_residual']:\n",
    "        model_name = f\"dexperts_steer_layer22_alphas_{alpha_base}_{alpha_expert}_{alpha_antiexpert}_{exp_config}\"\n",
    "\n",
    "        models[model_name] = {\n",
    "            'pos_path': POS_DIR / f'large_experts/negative/{exp_config}/dexperts_steer/steering_large_gpt2/alphas_{alpha_base}_{alpha_expert}_{alpha_antiexpert}/layer_22_freeze_emb_lm_head_with_expert_epochs100_train_val_sst/combine_at_layer/prompted_gens_dexperts-steer.jsonl'\n",
    "        }\n",
    "\n",
    "pos_prompts_res = read_sentiment_results({m: p['pos_path'] for m,p in models.items()}, auto_eval=True)\n",
    "    \n",
    "negative_steering_res = {}\n",
    "\n",
    "for model in pos_prompts_res.keys():\n",
    "    #print(model)\n",
    "    negative_steering_res[model] = {\n",
    "        #'neutral_prompts': neutral_prompts_res[model]['positive_proportion']*100,\n",
    "        'pos_prompts': pos_prompts_res[model]['positive_proportion']*100,\n",
    "        'dist-1': pos_prompts_res[model]['dist-1'],\n",
    "        'dist-2': pos_prompts_res[model]['dist-2'],\n",
    "        'dist-3': pos_prompts_res[model]['dist-3'],\n",
    "        'perplexity': pos_prompts_res[model]['perplexity'],\n",
    "    }\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(pd.DataFrame(negative_steering_res).transpose().sort_values(by='pos_prompts', ascending=False).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-47df9de8f3bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mconfig_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlayer_nums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0malpha_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mbeta_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpos_prompt_perfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_prompt_perf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 's'"
     ]
    }
   ],
   "source": [
    "config_names, layer_nums, alpha_vals, beta_vals, pos_prompt_perfs = [], [], [], [], []\n",
    "for model_name in negative_steering_res:\n",
    "    config_name = model_name[:model_name.find('layer')-1]\n",
    "    layer_num = model_name[model_name.find('layer')+5:].split('_')[0]\n",
    "    alpha_val = model_name[model_name.find('alpha')+5:].split('_')[0]\n",
    "    beta_val = model_name[model_name.find('beta')+4:].split('_')[0]\n",
    "    pos_prompt_perf = negative_steering_res[model_name]['pos_prompts']\n",
    "    #print(model_name)\n",
    "    #print(layer_num)\n",
    "    config_names.append(config_name)\n",
    "    layer_nums.append(int(layer_num))\n",
    "    alpha_vals.append(float(alpha_val))\n",
    "    beta_vals.append(float(beta_val))\n",
    "    pos_prompt_perfs.append(pos_prompt_perf)\n",
    "\n",
    "\n",
    "new_result_df = pd.DataFrame(list(zip(config_names, layer_nums, alpha_vals, beta_vals, pos_prompt_perfs)),\n",
    "                             columns =['config name', 'layer', 'alpha', 'beta', 'percent positive (lower better)'])\n",
    "\n",
    "layer_nums = [2, 12, 22, 32]\n",
    "for config in ['dexperts_steer', 'dexperts_steer_anti_only', 'dexperts_steer_expert_only']:\n",
    "    for layer_num in [2, 12, 22, 32]:\n",
    "        filtered_df = new_result_df[new_result_df['config name'] == config]\n",
    "        curr_df = filtered_df[filtered_df['layer'] == layer_num]# and new_result_df['layer'] == 22)\n",
    "\n",
    "        curr_mat = curr_df['percent positive (lower better)'].to_numpy().reshape(3, -1)\n",
    "        #print(curr_mat)\n",
    "        #new_result_df[new_result_df[new_result_df['config name'] == 'dexperts_steer']['layer'] == 22]\n",
    "        ax = sns.heatmap(curr_mat.transpose(), \n",
    "                    xticklabels=[1.0, 2.0, 3.0], \n",
    "                    yticklabels=[1.0, 2.0, 3.0, 3.2, 4.0], \n",
    "                    vmin=0, \n",
    "                    vmax=100, \n",
    "                    square=True, \n",
    "                    annot=True)\n",
    "        plt.title(f'{config}; layer{layer_num}', fontsize=15) # title with fontsize 20\n",
    "        plt.xlabel('Alpha', fontsize = 15) # x-axis label with fontsize 15\n",
    "        plt.ylabel('Beta', fontsize = 15) # y-axis label with fontsize 15\n",
    "\n",
    "        plt.show()\n",
    "#s.set(xlabel='bet', ylabel='Y-Axis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_prompts</th>\n",
       "      <th>dist-1</th>\n",
       "      <th>dist-2</th>\n",
       "      <th>dist-3</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_expert_only_layer32_alpha1.0_beta4.0</th>\n",
       "      <td>48.31</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.83</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_anti_only_layer32_alpha1.0_beta4.0</th>\n",
       "      <td>44.30</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.84</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_layer32_alpha1.0_beta4.0</th>\n",
       "      <td>43.53</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.84</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_expert_only_layer22_alpha2.0_beta2.0</th>\n",
       "      <td>33.56</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.83</td>\n",
       "      <td>202.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dexperts_steer_expert_only_layer22_alpha3.0_beta3.0</th>\n",
       "      <td>26.74</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.81</td>\n",
       "      <td>759.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    pos_prompts  dist-1  \\\n",
       "dexperts_steer_expert_only_layer32_alpha1.0_bet...        48.31    0.96   \n",
       "dexperts_steer_anti_only_layer32_alpha1.0_beta4.0         44.30    0.98   \n",
       "dexperts_steer_layer32_alpha1.0_beta4.0                   43.53    0.97   \n",
       "dexperts_steer_expert_only_layer22_alpha2.0_bet...        33.56    0.65   \n",
       "dexperts_steer_expert_only_layer22_alpha3.0_bet...        26.74    0.68   \n",
       "\n",
       "                                                    dist-2  dist-3  perplexity  \n",
       "dexperts_steer_expert_only_layer32_alpha1.0_bet...    0.92    0.83         NaN  \n",
       "dexperts_steer_anti_only_layer32_alpha1.0_beta4.0     0.92    0.84         NaN  \n",
       "dexperts_steer_layer32_alpha1.0_beta4.0               0.92    0.84         NaN  \n",
       "dexperts_steer_expert_only_layer22_alpha2.0_bet...    0.86    0.83      202.77  \n",
       "dexperts_steer_expert_only_layer22_alpha3.0_bet...    0.86    0.81      759.10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Evaluate the following\n",
    "\n",
    "expert_only layer 32; alpha 1.0, beta 4.0\n",
    "expert_only layer 22; alpha 3.0, beta 3.0\n",
    "expert_only layer 22; alpha 2.0, beta 2.0\n",
    "antiexpert_only layer 32; alpha 1.0, beta 4.0\n",
    "dexperts steer layer 32; alpha 1.0, beta 4.0\n",
    "'''\n",
    "\n",
    "POS_DIR = Path('../generations/sentiment/positive_prompts/')\n",
    "\n",
    "# alphas = [1.0, 2.0, 3.0]\n",
    "# betas = [1.0, 2.0, 3.0, 3.2, 4.0]\n",
    "# layer_nums = [2, 12, 22, 32]\n",
    "# configs = ['dexperts_steer', 'dexperts_steer_anti_only', 'dexperts_steer_expert_only']\n",
    "\n",
    "final_evaluated_models = {\n",
    "    'dexperts_steer_expert_only_layer32_alpha1.0_beta4.0': {\n",
    "        'pos_path': POS_DIR / f'large_experts/negative/dexperts_steer_expert_only/steering_large_gpt2/alpha_1.0_beta_4.0/layer_32_freeze_emb_lm_head_epochs100_train_val_sst/combine_at_layer/prompted_gens_dexperts-steer.jsonl'\n",
    "    },\n",
    "    'dexperts_steer_expert_only_layer22_alpha3.0_beta3.0': {\n",
    "        'pos_path': POS_DIR / f'large_experts/negative/dexperts_steer_expert_only/steering_large_gpt2/alpha_3.0_beta_3.0/layer_22_freeze_emb_lm_head_epochs100_train_val_sst/combine_at_layer/prompted_gens_dexperts-steer.jsonl'\n",
    "    },\n",
    "    'dexperts_steer_expert_only_layer22_alpha2.0_beta2.0': {\n",
    "        'pos_path': POS_DIR / f'large_experts/negative/dexperts_steer_expert_only/steering_large_gpt2/alpha_2.0_beta_2.0/layer_22_freeze_emb_lm_head_epochs100_train_val_sst/combine_at_layer/prompted_gens_dexperts-steer.jsonl'\n",
    "    },\n",
    "    'dexperts_steer_anti_only_layer32_alpha1.0_beta4.0': {\n",
    "        'pos_path': POS_DIR / f'large_experts/negative/dexperts_steer_anti_only/steering_large_gpt2/alpha_1.0_beta_4.0/layer_32_freeze_emb_lm_head_epochs100_train_val_sst/combine_at_layer/prompted_gens_dexperts-steer.jsonl'\n",
    "    },\n",
    "    'dexperts_steer_layer32_alpha1.0_beta4.0': {\n",
    "        'pos_path': POS_DIR / f'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_1.0_beta_4.0/layer_32_freeze_emb_lm_head_with_expert_epochs100_train_val_sst/combine_at_layer/prompted_gens_dexperts-steer.jsonl'\n",
    "    },\n",
    "}\n",
    "\n",
    "final_pos_prompts_res = read_sentiment_results({m: p['pos_path'] for m,p in final_evaluated_models.items()}, auto_eval=True)\n",
    "    \n",
    "final_negative_steering_res = {}\n",
    "\n",
    "for model in final_pos_prompts_res.keys():\n",
    "    final_negative_steering_res[model] = {\n",
    "        #'neutral_prompts': neutral_prompts_res[model]['positive_proportion']*100,\n",
    "        'pos_prompts': final_pos_prompts_res[model]['positive_proportion']*100,\n",
    "        'dist-1': final_pos_prompts_res[model]['dist-1'],\n",
    "        'dist-2': final_pos_prompts_res[model]['dist-2'],\n",
    "        'dist-3': final_pos_prompts_res[model]['dist-3'],\n",
    "        'perplexity': final_pos_prompts_res[model]['perplexity'],\n",
    "#         'dist-1': weighted_average(neutral_prompts_res, pos_prompts_res, 'dist-1'),\n",
    "#         'dist-2': weighted_average(neutral_prompts_res, pos_prompts_res, 'dist-2'),\n",
    "#         'dist-3': weighted_average(neutral_prompts_res, pos_prompts_res, 'dist-3'),\n",
    "#         'perplexity': weighted_average(neutral_prompts_res, pos_prompts_res, 'perplexity'),\n",
    "    }\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(pd.DataFrame(final_negative_steering_res).transpose().sort_values(by='pos_prompts', ascending=False).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dexperts_steer\n",
      "2\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_str = \"dexperts_steer_layer2_alpha1.0_beta1.0\"\n",
    "config_name = test_str[:test_str.find('layer')-1]\n",
    "layer_num = test_str[test_str.find('layer')+5:].split('_')[0]\n",
    "alpha_val = test_str[test_str.find('alpha')+5:].split('_')[0]\n",
    "beta_val = test_str[test_str.find('beta')+4:].split('_')[0]\n",
    "print(config_name)\n",
    "print(layer_num)\n",
    "print(alpha_val)\n",
    "print(beta_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.26it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  6.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# results corresponding to the bottom  half of Table 3\n",
    "\n",
    "NEUTRAL_DIR = Path('../generations/sentiment/neutral_prompts/')\n",
    "POS_DIR = Path('../generations/sentiment/positive_prompts/')\n",
    "\n",
    "models = {\n",
    "    'Layer 24 (anti-only)': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'large_experts/negative/dexperts_steer_anti_only/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'pos_path': POS_DIR / 'large_experts/negative/dexperts_steer_anti_only/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "    },\n",
    "    'Layer 24': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head_with_expert/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'pos_path': POS_DIR / 'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head_with_expert/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "    },\n",
    "    'Layer 24 combine at logit': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head_with_expert/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'pos_path': POS_DIR / 'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head_with_expert/combine_at_logit/prompted_gens_dexperts.jsonl',\n",
    "    }\n",
    "}\n",
    "\n",
    "#neutral_prompts_res = read_sentiment_results({m: p['neutral_path'] for m,p in models.items()})\n",
    "pos_prompts_res = read_sentiment_results({m: p['pos_path'] for m,p in models.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neutral_prompts</th>\n",
       "      <th>pos_prompts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Layer 24 (anti-only)</th>\n",
       "      <td>42.61</td>\n",
       "      <td>91.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer 24</th>\n",
       "      <td>36.19</td>\n",
       "      <td>90.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer 24 combine at logit</th>\n",
       "      <td>36.19</td>\n",
       "      <td>91.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           neutral_prompts  pos_prompts\n",
       "Layer 24 (anti-only)                 42.61        91.05\n",
       "Layer 24                             36.19        90.71\n",
       "Layer 24 combine at logit            36.19        91.46"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_steering_res = {}\n",
    "assert set(neutral_prompts_res.keys()) == set(pos_prompts_res.keys())\n",
    "for model in neutral_prompts_res.keys():\n",
    "    negative_steering_res[model] = {\n",
    "        #'neutral_prompts': neutral_prompts_res[model]['positive_proportion']*100,\n",
    "        'pos_prompts': pos_prompts_res[model]['positive_proportion']*100,\n",
    "#         'dist-1': weighted_average(neutral_prompts_res, pos_prompts_res, 'dist-1'),\n",
    "#         'dist-2': weighted_average(neutral_prompts_res, pos_prompts_res, 'dist-2'),\n",
    "#         'dist-3': weighted_average(neutral_prompts_res, pos_prompts_res, 'dist-3'),\n",
    "#         'perplexity': weighted_average(neutral_prompts_res, pos_prompts_res, 'perplexity'),\n",
    "    }\n",
    "pd.DataFrame(negative_steering_res).transpose().sort_values(by='neutral_prompts', ascending=False).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alpha experiment, I think fluency is really bad for high alphas and make less logical sense on viewing some of the generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  5.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_prompts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_0.5</th>\n",
       "      <td>94.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_1.0</th>\n",
       "      <td>93.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_1.5</th>\n",
       "      <td>93.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_2.0</th>\n",
       "      <td>92.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_2.5</th>\n",
       "      <td>91.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_3.0</th>\n",
       "      <td>91.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_3.2</th>\n",
       "      <td>90.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_3.5</th>\n",
       "      <td>90.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_4.0</th>\n",
       "      <td>89.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_5.0</th>\n",
       "      <td>87.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_10.0</th>\n",
       "      <td>77.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_20.0</th>\n",
       "      <td>61.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_24_alpha_30.0</th>\n",
       "      <td>48.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pos_prompts\n",
       "Layer_24_alpha_0.5         94.64\n",
       "Layer_24_alpha_1.0         93.97\n",
       "Layer_24_alpha_1.5         93.32\n",
       "Layer_24_alpha_2.0         92.59\n",
       "Layer_24_alpha_2.5         91.89\n",
       "Layer_24_alpha_3.0         91.09\n",
       "Layer_24_alpha_3.2         90.71\n",
       "Layer_24_alpha_3.5         90.25\n",
       "Layer_24_alpha_4.0         89.40\n",
       "Layer_24_alpha_5.0         87.35\n",
       "Layer_24_alpha_10.0        77.67\n",
       "Layer_24_alpha_20.0        61.13\n",
       "Layer_24_alpha_30.0        48.11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results corresponding to the bottom  half of Table 3\n",
    "\n",
    "POS_DIR = Path('../generations/sentiment/positive_prompts/')\n",
    "\n",
    "models = {}\n",
    "for alpha in [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.2, 3.5, 4.0, 5.0, 10.0, 20.0, 30.0]:\n",
    "    models[f'Layer_24_alpha_{alpha}'] = {\n",
    "        'pos_path': POS_DIR / f'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_{alpha}/layer_24_freeze_emb_lm_head_with_expert/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "    }\n",
    "\n",
    "pos_prompts_res = read_sentiment_results({m: p['pos_path'] for m,p in models.items()})\n",
    "negative_steering_res = {}\n",
    "#assert set(neutral_prompts_res.keys()) == set(pos_prompts_res.keys())\n",
    "for model in pos_prompts_res.keys():\n",
    "    negative_steering_res[model] = {\n",
    "        #'neutral_prompts': neutral_prompts_res[model]['positive_proportion']*100,\n",
    "        'pos_prompts': pos_prompts_res[model]['positive_proportion']*100,\n",
    "#         'dist-1': weighted_average(neutral_prompts_res, pos_prompts_res, 'dist-1'),\n",
    "#         'dist-2': weighted_average(neutral_prompts_res, pos_prompts_res, 'dist-2'),\n",
    "#         'dist-3': weighted_average(neutral_prompts_res, pos_prompts_res, 'dist-3'),\n",
    "#         'perplexity': weighted_average(neutral_prompts_res, pos_prompts_res, 'perplexity'),\n",
    "    }\n",
    "pd.DataFrame(negative_steering_res).transpose().sort_values(by='pos_prompts', ascending=False).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "def return_perplexity_of_sentiment_model(models_dict: Dict[str, str]):\n",
    "    res = {}\n",
    "    for model in tqdm(models_dict):\n",
    "        path_to_model = models_dict[model]\n",
    "        if path_to_model == 'None':\n",
    "            res[model] = {'ppl': -1}\n",
    "        else:\n",
    "            with open(os.path.join(path_to_model, 'eval_results_lm.txt')) as f:\n",
    "                for line in f:\n",
    "                    res[model] = {'ppl': float(line.split(' = ')[1])}\n",
    "                    break\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.27it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16690.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 24892.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_prompts</th>\n",
       "      <th>antiexpert_ppl</th>\n",
       "      <th>expert_ppl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DExperts (anti-only)</th>\n",
       "      <td>92.29</td>\n",
       "      <td>18.81</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer 24 (anti-only) 3 epochs</th>\n",
       "      <td>91.05</td>\n",
       "      <td>42.09</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DExperts small experts (anti-only)</th>\n",
       "      <td>90.93</td>\n",
       "      <td>40.04</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer 24 3 epochs</th>\n",
       "      <td>90.71</td>\n",
       "      <td>42.09</td>\n",
       "      <td>43.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer 24 100 epochs</th>\n",
       "      <td>68.21</td>\n",
       "      <td>7.69</td>\n",
       "      <td>7.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer 24 50 epochs</th>\n",
       "      <td>64.32</td>\n",
       "      <td>18.01</td>\n",
       "      <td>17.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer 24 (anti-only) 100 epochs</th>\n",
       "      <td>56.00</td>\n",
       "      <td>7.69</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DExperts small experts</th>\n",
       "      <td>46.55</td>\n",
       "      <td>40.04</td>\n",
       "      <td>41.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer 24 (anti-only) 50 epochs</th>\n",
       "      <td>40.44</td>\n",
       "      <td>18.01</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DExperts (large)</th>\n",
       "      <td>33.95</td>\n",
       "      <td>18.81</td>\n",
       "      <td>18.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    pos_prompts  antiexpert_ppl  expert_ppl\n",
       "DExperts (anti-only)                      92.29           18.81       -1.00\n",
       "Layer 24 (anti-only) 3 epochs             91.05           42.09       -1.00\n",
       "DExperts small experts (anti-only)        90.93           40.04       -1.00\n",
       "Layer 24 3 epochs                         90.71           42.09       43.80\n",
       "Layer 24 100 epochs                       68.21            7.69        7.20\n",
       "Layer 24 50 epochs                        64.32           18.01       17.93\n",
       "Layer 24 (anti-only) 100 epochs           56.00            7.69       -1.00\n",
       "DExperts small experts                    46.55           40.04       41.09\n",
       "Layer 24 (anti-only) 50 epochs            40.44           18.01       -1.00\n",
       "DExperts (large)                          33.95           18.81       18.58"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Different Language Model experiments\n",
    "\n",
    "# results corresponding to the bottom  half of Table 3\n",
    "\n",
    "POS_DIR = Path('../generations/sentiment/positive_prompts/')\n",
    "model_dir = '../models/experts/sentiment/'\n",
    "source_sentiment = 'positive'\n",
    "target_sentiment = 'negative'\n",
    "\n",
    "models = {\n",
    "    'DExperts (anti-only)': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'large_experts/negative/dexperts_anti_only/prompted_gens_dexperts.jsonl',\n",
    "        'pos_path': POS_DIR / 'large_experts/negative/dexperts_anti_only/prompted_gens_dexperts.jsonl',\n",
    "        'antiexpert_ppl_path': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}\",\n",
    "        'expert_ppl_path': 'None',\n",
    "    },\n",
    "    'DExperts (large)': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'large_experts/negative/dexperts/prompted_gens_dexperts.jsonl',\n",
    "        'pos_path': POS_DIR / 'large_experts/negative/dexperts/prompted_gens_dexperts.jsonl',\n",
    "        'antiexpert_ppl_path': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}\",\n",
    "        'expert_ppl_path': f\"{model_dir}/large/finetuned_gpt2_{target_sentiment}\",\n",
    "    },\n",
    "    'DExperts small experts (anti-only)': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'small_experts/negative/dexperts_anti_only/prompted_gens_dexperts.jsonl',\n",
    "        'pos_path': POS_DIR / 'small_experts/negative/dexperts_anti_only/prompted_gens_dexperts.jsonl',\n",
    "        'antiexpert_ppl_path': f\"{model_dir}/small/finetuned_gpt2_{source_sentiment}\",\n",
    "        'expert_ppl_path': 'None',\n",
    "    },\n",
    "    'DExperts small experts': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'small_experts/negative/dexperts/prompted_gens_dexperts.jsonl',\n",
    "        'pos_path': POS_DIR / 'small_experts/negative/dexperts/prompted_gens_dexperts.jsonl',\n",
    "        'antiexpert_ppl_path': f\"{model_dir}/small/finetuned_gpt2_{source_sentiment}\",\n",
    "        'expert_ppl_path': f\"{model_dir}/small/finetuned_gpt2_{target_sentiment}\",\n",
    "    },\n",
    "    'Layer 24 (anti-only) 3 epochs': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'large_experts/negative/dexperts_steer_anti_only/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'pos_path': POS_DIR / 'large_experts/negative/dexperts_steer_anti_only/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'antiexpert_ppl_path': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}_experimental_freeze_emb_and_lmhead_layers24\",\n",
    "        'expert_ppl_path': 'None',\n",
    "    },\n",
    "    'Layer 24 3 epochs': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head_with_expert/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'pos_path': POS_DIR / 'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head_with_expert/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'antiexpert_ppl_path': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}_experimental_freeze_emb_and_lmhead_layers24\",\n",
    "        'expert_ppl_path': f\"{model_dir}/large/finetuned_gpt2_{target_sentiment}_experimental_freeze_emb_and_lmhead_layers24\",\n",
    "    },\n",
    "    'Layer 24 (anti-only) 50 epochs': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'large_experts/negative/dexperts_steer_anti_only/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'pos_path': POS_DIR / 'large_experts/negative/dexperts_steer_anti_only/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head_epochs50/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'antiexpert_ppl_path': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}_experimental_freeze_emb_and_lmhead_layers24_50\",\n",
    "        'expert_ppl_path': 'None',\n",
    "    },\n",
    "    'Layer 24 50 epochs': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head_with_expert/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'pos_path': POS_DIR / 'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head_with_expert_epochs50/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'antiexpert_ppl_path': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}_experimental_freeze_emb_and_lmhead_layers24_50\",\n",
    "        'expert_ppl_path': f\"{model_dir}/large/finetuned_gpt2_{target_sentiment}_experimental_freeze_emb_and_lmhead_layers24_50\",\n",
    "    },\n",
    "    'Layer 24 (anti-only) 100 epochs': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'large_experts/negative/dexperts_steer_anti_only/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'pos_path': POS_DIR / 'large_experts/negative/dexperts_steer_anti_only/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head_epochs100/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'antiexpert_ppl_path': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}_experimental_freeze_emb_and_lmhead_layers24_100\",\n",
    "        'expert_ppl_path': 'None',\n",
    "    },\n",
    "    'Layer 24 100 epochs': {\n",
    "        #'neutral_path': NEUTRAL_DIR / 'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head_with_expert/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'pos_path': POS_DIR / 'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_3.2/layer_24_freeze_emb_lm_head_with_expert_epochs100/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'antiexpert_ppl_path': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}_experimental_freeze_emb_and_lmhead_layers24_100\",\n",
    "        'expert_ppl_path': f\"{model_dir}/large/finetuned_gpt2_{target_sentiment}_experimental_freeze_emb_and_lmhead_layers24_100\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# read sentiment control results\n",
    "pos_prompts_res = read_sentiment_results({m: p['pos_path'] for m,p in models.items()})\n",
    "antiexpert_ppl_res = return_perplexity_of_sentiment_model({m: p['antiexpert_ppl_path'] for m,p in models.items()})\n",
    "expert_ppl_res = return_perplexity_of_sentiment_model({m: p['expert_ppl_path'] for m,p in models.items()})\n",
    "\n",
    "negative_steering_res = {}\n",
    "for model in pos_prompts_res.keys():\n",
    "    negative_steering_res[model] = {\n",
    "        'pos_prompts': pos_prompts_res[model]['positive_proportion']*100,\n",
    "        'antiexpert_ppl': antiexpert_ppl_res[model]['ppl'],\n",
    "        'expert_ppl': expert_ppl_res[model]['ppl'],\n",
    "    }\n",
    "    \n",
    "pd.DataFrame(negative_steering_res).transpose().sort_values(by='pos_prompts', ascending=False).round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ppl': 7.69039240771082}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos -> neg\n",
    "model_dir = '../models/experts/sentiment/'\n",
    "source_sentiment = 'positive'\n",
    "target_sentiment = 'negative'\n",
    "model_path_to_expert_ppls = {\n",
    "    'DExperts (anti-only)': 'None'#f\"{model_dir}/large/finetuned_gpt2_{target_sentiment}\",\n",
    "    'Layer 24 (anti-only) 3 epochs': 'None'#f\"{model_dir}/large/finetuned_gpt2_{target_sentiment}_experimental_freeze_emb_and_lmhead_layers24\",\n",
    "    'DExperts small experts (anti-only)': 'None'#f\"{model_dir}/small/finetuned_gpt2_{target_sentiment}\",\n",
    "    'Layer 24 3 epochs': f\"{model_dir}/large/finetuned_gpt2_{target_sentiment}_experimental_freeze_emb_and_lmhead_layers24\",\n",
    "    'Layer 24 100 epochs': f\"{model_dir}/large/finetuned_gpt2_{target_sentiment}_experimental_freeze_emb_and_lmhead_layers24_50\",\n",
    "    'Layer 24 50 epochs': f\"{model_dir}/large/finetuned_gpt2_{target_sentiment}_experimental_freeze_emb_and_lmhead_layers24_100\",\n",
    "    'Layer 24 (anti-only) 100 epochs': 'None'#f\"{model_dir}/large/finetuned_gpt2_{target_sentiment}_experimental_freeze_emb_and_lmhead_layers24_100\",\n",
    "    'DExperts small experts': f\"{model_dir}/small/finetuned_gpt2_{target_sentiment}\",\n",
    "    'Layer 24 (anti-only) 50 epochs': 'None',#f\"{model_dir}/large/finetuned_gpt2_{target_sentiment}_experimental_freeze_emb_and_lmhead_layers24_50\",\n",
    "    'DExperts (large)': f\"{model_dir}/large/finetuned_gpt2_{target_sentiment}\",\n",
    "}\n",
    "\n",
    "model_path_to_antiexpert_ppls = {\n",
    "    'DExperts (anti-only)': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}\",\n",
    "    'Layer 24 (anti-only) 3 epochs': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}_experimental_freeze_emb_and_lmhead_layers24\",\n",
    "    'DExperts small experts (anti-only)': f\"{model_dir}/small/finetuned_gpt2_{source_sentiment}\",\n",
    "    'Layer 24 3 epochs': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}_experimental_freeze_emb_and_lmhead_layers24\",\n",
    "    'Layer 24 100 epochs': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}_experimental_freeze_emb_and_lmhead_layers24_50\",\n",
    "    'Layer 24 50 epochs': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}_experimental_freeze_emb_and_lmhead_layers24_100\",\n",
    "    'Layer 24 (anti-only) 100 epochs': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}_experimental_freeze_emb_and_lmhead_layers24_100\",\n",
    "    'DExperts small experts': f\"{model_dir}/small/finetuned_gpt2_{source_sentiment}\",\n",
    "    'Layer 24 (anti-only) 50 epochs': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}_experimental_freeze_emb_and_lmhead_layers24_50\",\n",
    "    'DExperts (large)': f\"{model_dir}/large/finetuned_gpt2_{source_sentiment}\",\n",
    "}\n",
    "\n",
    "pos_prompts_expert_ppl_res = return_perplexity_of_sentiment_model({m: p['pos_path'] for m,p in models.items()})\n",
    "\n",
    "negative_steering_res = {}\n",
    "for model in pos_prompts_res.keys():\n",
    "    negative_steering_res[model] = {\n",
    "        'pos_prompts': pos_prompts_res[model]['positive_proportion']*100,\n",
    "    }\n",
    "    \n",
    "pd.DataFrame(negative_steering_res).transpose().sort_values(by='pos_prompts', ascending=False).round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:00<00:00, 4554.08it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 32362.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive_ppl</th>\n",
       "      <th>negative_ppl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPT2 large</th>\n",
       "      <td>31.35</td>\n",
       "      <td>30.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT2 small</th>\n",
       "      <td>42.89</td>\n",
       "      <td>43.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer0</th>\n",
       "      <td>34.50</td>\n",
       "      <td>33.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer1</th>\n",
       "      <td>33.73</td>\n",
       "      <td>32.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer2</th>\n",
       "      <td>33.62</td>\n",
       "      <td>32.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer3</th>\n",
       "      <td>33.65</td>\n",
       "      <td>32.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer4</th>\n",
       "      <td>34.10</td>\n",
       "      <td>33.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer5</th>\n",
       "      <td>33.85</td>\n",
       "      <td>33.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer6</th>\n",
       "      <td>33.72</td>\n",
       "      <td>32.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer7</th>\n",
       "      <td>33.66</td>\n",
       "      <td>33.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer8</th>\n",
       "      <td>34.07</td>\n",
       "      <td>33.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer9</th>\n",
       "      <td>34.05</td>\n",
       "      <td>33.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer10</th>\n",
       "      <td>33.93</td>\n",
       "      <td>33.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer11</th>\n",
       "      <td>34.19</td>\n",
       "      <td>33.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer12</th>\n",
       "      <td>34.23</td>\n",
       "      <td>33.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer13</th>\n",
       "      <td>34.24</td>\n",
       "      <td>33.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer14</th>\n",
       "      <td>34.33</td>\n",
       "      <td>33.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer15</th>\n",
       "      <td>34.63</td>\n",
       "      <td>33.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer16</th>\n",
       "      <td>34.64</td>\n",
       "      <td>33.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer17</th>\n",
       "      <td>34.60</td>\n",
       "      <td>33.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer18</th>\n",
       "      <td>34.54</td>\n",
       "      <td>33.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer19</th>\n",
       "      <td>34.62</td>\n",
       "      <td>34.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer20</th>\n",
       "      <td>34.48</td>\n",
       "      <td>33.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer21</th>\n",
       "      <td>34.36</td>\n",
       "      <td>33.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer22</th>\n",
       "      <td>34.61</td>\n",
       "      <td>34.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer23</th>\n",
       "      <td>34.77</td>\n",
       "      <td>34.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer24</th>\n",
       "      <td>34.82</td>\n",
       "      <td>34.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer25</th>\n",
       "      <td>35.25</td>\n",
       "      <td>34.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer26</th>\n",
       "      <td>35.21</td>\n",
       "      <td>35.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer27</th>\n",
       "      <td>35.15</td>\n",
       "      <td>35.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer28</th>\n",
       "      <td>35.74</td>\n",
       "      <td>35.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer29</th>\n",
       "      <td>36.21</td>\n",
       "      <td>35.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer30</th>\n",
       "      <td>36.21</td>\n",
       "      <td>36.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer31</th>\n",
       "      <td>36.27</td>\n",
       "      <td>36.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer32</th>\n",
       "      <td>36.71</td>\n",
       "      <td>36.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer33</th>\n",
       "      <td>37.00</td>\n",
       "      <td>37.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer34</th>\n",
       "      <td>37.28</td>\n",
       "      <td>37.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer35</th>\n",
       "      <td>37.67</td>\n",
       "      <td>37.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            positive_ppl  negative_ppl\n",
       "GPT2 large         31.35         30.51\n",
       "GPT2 small         42.89         43.15\n",
       "layer0             34.50         33.45\n",
       "layer1             33.73         32.80\n",
       "layer2             33.62         32.81\n",
       "layer3             33.65         32.94\n",
       "layer4             34.10         33.36\n",
       "layer5             33.85         33.04\n",
       "layer6             33.72         32.96\n",
       "layer7             33.66         33.09\n",
       "layer8             34.07         33.48\n",
       "layer9             34.05         33.28\n",
       "layer10            33.93         33.44\n",
       "layer11            34.19         33.65\n",
       "layer12            34.23         33.93\n",
       "layer13            34.24         33.69\n",
       "layer14            34.33         33.80\n",
       "layer15            34.63         33.74\n",
       "layer16            34.64         33.97\n",
       "layer17            34.60         33.96\n",
       "layer18            34.54         33.96\n",
       "layer19            34.62         34.10\n",
       "layer20            34.48         33.99\n",
       "layer21            34.36         33.64\n",
       "layer22            34.61         34.14\n",
       "layer23            34.77         34.37\n",
       "layer24            34.82         34.52\n",
       "layer25            35.25         34.93\n",
       "layer26            35.21         35.15\n",
       "layer27            35.15         35.55\n",
       "layer28            35.74         35.90\n",
       "layer29            36.21         35.88\n",
       "layer30            36.21         36.67\n",
       "layer31            36.27         36.55\n",
       "layer32            36.71         36.98\n",
       "layer33            37.00         37.44\n",
       "layer34            37.28         37.45\n",
       "layer35            37.67         37.78"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def return_perplexity_of_sentiment_model(models_dict: Dict[str, str]):\n",
    "    res = {}\n",
    "    for model in tqdm(models_dict):\n",
    "        path_to_model = models_dict[model]\n",
    "        if path_to_model == 'None':\n",
    "            res[model] = {'ppl': -1}\n",
    "        else:\n",
    "            with open(os.path.join(path_to_model, 'eval_results_lm.txt')) as f:\n",
    "                for line in f:\n",
    "                    res[model] = {'ppl': float(line.split(' = ')[1])}\n",
    "                    break\n",
    "    return res\n",
    "\n",
    "\n",
    "POS_DIR = Path('../generations/sentiment/positive_prompts/')\n",
    "model_dir = '../models/experts/sentiment/'\n",
    "source_sentiment = 'positive'\n",
    "target_sentiment = 'negative'\n",
    "\n",
    "models = {\n",
    "    'GPT2 large': {\n",
    "        'positive_path': f'{model_dir}/large/finetuned_gpt2_train_val_sst_positive_100_maxepochs',\n",
    "        'negative_path': f'{model_dir}/large/finetuned_gpt2_train_val_sst_negative_100_maxepochs',\n",
    "    },\n",
    "    'GPT2 small': {\n",
    "        'positive_path': f'{model_dir}/small/finetuned_gpt2_train_val_sst_positive_100_maxepochs',\n",
    "        'negative_path': f'{model_dir}/small/finetuned_gpt2_train_val_sst_negative_100_maxepochs',\n",
    "    }\n",
    "}\n",
    "\n",
    "for i in range(0, 36):\n",
    "    models[f'layer{i}'] = {\n",
    "        'positive_path': f'{model_dir}/large/finetuned_gpt2_train_val_sst_positive_experimental_freeze_emb_and_lmhead_layers{i}_100_maxepochs',\n",
    "        'negative_path': f'{model_dir}/large/finetuned_gpt2_train_val_sst_negative_experimental_freeze_emb_and_lmhead_layers{i}_100_maxepochs',\n",
    "    }\n",
    "\n",
    "positive_ppl_res = return_perplexity_of_sentiment_model({m: p['positive_path'] for m,p in models.items()})\n",
    "negative_ppl_res = return_perplexity_of_sentiment_model({m: p['negative_path'] for m,p in models.items()})\n",
    "\n",
    "negative_steering_res = {}\n",
    "for model in positive_ppl_res.keys():\n",
    "    negative_steering_res[model] = {\n",
    "        'positive_ppl': positive_ppl_res[model]['ppl'],\n",
    "        'negative_ppl': negative_ppl_res[model]['ppl'],\n",
    "    }\n",
    "    \n",
    "pd.DataFrame(negative_steering_res).transpose().round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:03<00:00,  5.79it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 26934.52it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 23952.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_prompts</th>\n",
       "      <th>antiexpert_ppl</th>\n",
       "      <th>expert_ppl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DExperts Small (anti-only)</th>\n",
       "      <td>90.92</td>\n",
       "      <td>42.89</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DExperts (anti-only)</th>\n",
       "      <td>90.66</td>\n",
       "      <td>31.35</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_22 (anti-only)</th>\n",
       "      <td>86.86</td>\n",
       "      <td>34.61</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_32 (anti-only)</th>\n",
       "      <td>83.44</td>\n",
       "      <td>36.71</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_32</th>\n",
       "      <td>78.99</td>\n",
       "      <td>36.71</td>\n",
       "      <td>36.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_12 (anti-only)</th>\n",
       "      <td>77.67</td>\n",
       "      <td>34.23</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_2</th>\n",
       "      <td>69.26</td>\n",
       "      <td>33.62</td>\n",
       "      <td>32.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_2 (anti-only)</th>\n",
       "      <td>68.68</td>\n",
       "      <td>33.62</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_12</th>\n",
       "      <td>67.75</td>\n",
       "      <td>34.23</td>\n",
       "      <td>33.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_32 (expert-only)</th>\n",
       "      <td>63.69</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>36.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_22</th>\n",
       "      <td>58.88</td>\n",
       "      <td>34.61</td>\n",
       "      <td>34.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DExperts Small</th>\n",
       "      <td>53.70</td>\n",
       "      <td>42.89</td>\n",
       "      <td>43.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_12 (expert-only)</th>\n",
       "      <td>50.54</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>33.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DExperts (expert-only)</th>\n",
       "      <td>47.57</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>30.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DExperts Small (expert-only)</th>\n",
       "      <td>44.15</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>43.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_2 (expert-only)</th>\n",
       "      <td>43.43</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>32.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DExperts</th>\n",
       "      <td>38.00</td>\n",
       "      <td>31.35</td>\n",
       "      <td>30.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer_22 (expert-only)</th>\n",
       "      <td>26.74</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>34.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              pos_prompts  antiexpert_ppl  expert_ppl\n",
       "DExperts Small (anti-only)          90.92           42.89       -1.00\n",
       "DExperts (anti-only)                90.66           31.35       -1.00\n",
       "Layer_22 (anti-only)                86.86           34.61       -1.00\n",
       "Layer_32 (anti-only)                83.44           36.71       -1.00\n",
       "Layer_32                            78.99           36.71       36.98\n",
       "Layer_12 (anti-only)                77.67           34.23       -1.00\n",
       "Layer_2                             69.26           33.62       32.81\n",
       "Layer_2 (anti-only)                 68.68           33.62       -1.00\n",
       "Layer_12                            67.75           34.23       33.93\n",
       "Layer_32 (expert-only)              63.69           -1.00       36.98\n",
       "Layer_22                            58.88           34.61       34.14\n",
       "DExperts Small                      53.70           42.89       43.15\n",
       "Layer_12 (expert-only)              50.54           -1.00       33.93\n",
       "DExperts (expert-only)              47.57           -1.00       30.51\n",
       "DExperts Small (expert-only)        44.15           -1.00       43.15\n",
       "Layer_2 (expert-only)               43.43           -1.00       32.81\n",
       "DExperts                            38.00           31.35       30.51\n",
       "Layer_22 (expert-only)              26.74           -1.00       34.14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### New language models train_val_sst positive\n",
    "# results corresponding to the bottom  half of Table 3\n",
    "\n",
    "model_dir = '../models/experts/sentiment/'\n",
    "POS_DIR = Path('../generations/sentiment/positive_prompts/')\n",
    "\n",
    "models = {\n",
    "    \"DExperts Small\" : {\n",
    "        'pos_path': POS_DIR / f\"small_experts/negative/dexperts/train_val_sst/prompted_gens_dexperts.jsonl\",\n",
    "        'positive_ppl_path': f'{model_dir}/small/finetuned_gpt2_train_val_sst_positive_100_maxepochs',\n",
    "        'negative_ppl_path': f'{model_dir}/small/finetuned_gpt2_train_val_sst_negative_100_maxepochs',\n",
    "    },\n",
    "    \"DExperts Small (anti-only)\" : {\n",
    "        'pos_path': POS_DIR / f\"small_experts/negative/dexperts_anti_only/train_val_sst/prompted_gens_dexperts.jsonl\",\n",
    "        'positive_ppl_path': f'{model_dir}/small/finetuned_gpt2_train_val_sst_positive_100_maxepochs',\n",
    "        'negative_ppl_path': 'None',\n",
    "    },\n",
    "    \"DExperts Small (expert-only)\" : {\n",
    "        'pos_path': POS_DIR / f\"small_experts/negative/dexperts_expert_only/train_val_sst/prompted_gens_dexperts.jsonl\",\n",
    "        'positive_ppl_path': 'None',\n",
    "        'negative_ppl_path': f'{model_dir}/small/finetuned_gpt2_train_val_sst_negative_100_maxepochs',\n",
    "    },\n",
    "    \"DExperts\" : {\n",
    "        'pos_path': POS_DIR / f\"large_experts/negative/dexperts/train_val_sst/prompted_gens_dexperts.jsonl\",\n",
    "        'positive_ppl_path': f'{model_dir}/large/finetuned_gpt2_train_val_sst_positive_100_maxepochs',\n",
    "        'negative_ppl_path': f'{model_dir}/large/finetuned_gpt2_train_val_sst_negative_100_maxepochs',\n",
    "    },\n",
    "    \"DExperts (anti-only)\" : {\n",
    "        'pos_path': POS_DIR / f\"large_experts/negative/dexperts_anti_only/train_val_sst/prompted_gens_dexperts.jsonl\",\n",
    "        'positive_ppl_path': f'{model_dir}/large/finetuned_gpt2_train_val_sst_positive_100_maxepochs',\n",
    "        'negative_ppl_path': 'None',\n",
    "    },\n",
    "    \"DExperts (expert-only)\" : {\n",
    "        'pos_path': POS_DIR / f\"large_experts/negative/dexperts_expert_only/train_val_sst/prompted_gens_dexperts.jsonl\",\n",
    "        'positive_ppl_path': 'None', \n",
    "        'negative_ppl_path': f'{model_dir}/large/finetuned_gpt2_train_val_sst_negative_100_maxepochs',\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "for layer_num in [2, 12, 22, 32]:\n",
    "    models[f'Layer_{layer_num}'] = {\n",
    "        'pos_path': POS_DIR / f'large_experts/negative/dexperts_steer/steering_large_gpt2/alpha_3.2/layer_{layer_num}_freeze_emb_lm_head_with_expert_epochs100_train_val_sst/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'positive_ppl_path': f'{model_dir}/large/finetuned_gpt2_train_val_sst_positive_experimental_freeze_emb_and_lmhead_layers{layer_num}_100_maxepochs',\n",
    "        'negative_ppl_path': f'{model_dir}/large/finetuned_gpt2_train_val_sst_negative_experimental_freeze_emb_and_lmhead_layers{layer_num}_100_maxepochs',\n",
    "    }\n",
    "    \n",
    "    models[f'Layer_{layer_num} (anti-only)'] = {\n",
    "        'pos_path': POS_DIR / f'large_experts/negative/dexperts_steer_anti_only/steering_large_gpt2/alpha_3.2/layer_{layer_num}_freeze_emb_lm_head_epochs100_train_val_sst/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'positive_ppl_path': f'{model_dir}/large/finetuned_gpt2_train_val_sst_positive_experimental_freeze_emb_and_lmhead_layers{layer_num}_100_maxepochs',\n",
    "        'negative_ppl_path': 'None',\n",
    "    }\n",
    "    \n",
    "    models[f'Layer_{layer_num} (expert-only)'] = {\n",
    "        'pos_path': POS_DIR / f'large_experts/negative/dexperts_steer_expert_only/steering_large_gpt2/alpha_3.2/layer_{layer_num}_freeze_emb_lm_head_epochs100_train_val_sst/combine_at_layer/prompted_gens_dexperts-steer.jsonl',\n",
    "        'positive_ppl_path': 'None',\n",
    "        'negative_ppl_path': f'{model_dir}/large/finetuned_gpt2_train_val_sst_negative_experimental_freeze_emb_and_lmhead_layers{layer_num}_100_maxepochs',\n",
    "    }\n",
    "\n",
    "pos_prompts_res = read_sentiment_results({m: p['pos_path'] for m,p in models.items()})\n",
    "antiexpert_ppl_res = return_perplexity_of_sentiment_model({m: p['positive_ppl_path'] for m,p in models.items()})\n",
    "expert_ppl_res = return_perplexity_of_sentiment_model({m: p['negative_ppl_path'] for m,p in models.items()})\n",
    "negative_steering_res = {}\n",
    "#assert set(neutral_prompts_res.keys()) == set(pos_prompts_res.keys())\n",
    "for model in pos_prompts_res.keys():\n",
    "    negative_steering_res[model] = {\n",
    "        #'neutral_prompts': neutral_prompts_res[model]['positive_proportion']*100,\n",
    "        'pos_prompts': pos_prompts_res[model]['positive_proportion']*100,\n",
    "        'antiexpert_ppl': antiexpert_ppl_res[model]['ppl'],\n",
    "        'expert_ppl': expert_ppl_res[model]['ppl'],\n",
    "#         'dist-1': weighted_average(neutral_prompts_res, pos_prompts_res, 'dist-1'),\n",
    "#         'dist-2': weighted_average(neutral_prompts_res, pos_prompts_res, 'dist-2'),\n",
    "#         'dist-3': weighted_average(neutral_prompts_res, pos_prompts_res, 'dist-3'),\n",
    "#         'perplexity': weighted_average(neutral_prompts_res, pos_prompts_res, 'perplexity'),\n",
    "    }\n",
    "pd.DataFrame(negative_steering_res).transpose().sort_values(by='pos_prompts', ascending=False).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dexperts",
   "language": "python",
   "name": "dexperts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
